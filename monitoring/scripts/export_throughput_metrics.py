#!/usr/bin/env python3
"""
ç³»çµ±ååé‡æŒ‡æ¨™åŒ¯å‡ºå·¥å…·

æ­¤è…³æœ¬æœƒæŸ¥è©¢ Prometheus ä¸­ã€Œç³»çµ±ååé‡ (Throughput)ã€åœ–è¡¨çš„æ‰€æœ‰æŒ‡æ¨™è³‡æ–™ï¼Œ
ä¸¦å°‡çµæœåŒ¯å‡ºç‚º CSV æª”æ¡ˆã€‚

ä½¿ç”¨æ–¹å¼:
    python export_throughput_metrics.py --start "2024-11-25T00:00:00Z" --end "2024-11-25T23:59:59Z"
    æˆ–
    python export_throughput_metrics.py --duration 1h  # æœ€è¿‘ 1 å°æ™‚
    python export_throughput_metrics.py --duration 30m # æœ€è¿‘ 30 åˆ†é˜
"""

import argparse
import csv
import sys
import os  # æ–°å¢: ç”¨æ–¼æª”æ¡ˆæª¢æŸ¥
from datetime import datetime, timedelta
from typing import List, Dict, Any
import requests
from urllib.parse import urljoin
import pandas as pd  # æ–°å¢: ç”¨æ–¼è¨ˆç®—ä¸­ä½æ•¸å’Œè³‡æ–™ç¯©é¸
from pathlib import Path  # æ–°å¢: ç”¨æ–¼è·¯å¾‘è™•ç†

# Prometheus é€£ç·šè¨­å®š
PROMETHEUS_URL = "http://localhost:9090"


class PrometheusExporter:
    """Prometheus æŒ‡æ¨™æŸ¥è©¢èˆ‡åŒ¯å‡ºå·¥å…·"""

    def __init__(self, prometheus_url: str = PROMETHEUS_URL):
        self.prometheus_url = prometheus_url
        self.query_url = urljoin(prometheus_url, "/api/v1/query_range")

    def query_range(self, query: str, start: datetime, end: datetime, step: str = "1s") -> Dict[str, Any]:
        """
        æŸ¥è©¢ Prometheus æ™‚é–“ç¯„åœè³‡æ–™

        Args:
            query: PromQL æŸ¥è©¢èªå¥
            start: é–‹å§‹æ™‚é–“
            end: çµæŸæ™‚é–“
            step: æ™‚é–“é–“éš” (é è¨­ 1s)

        Returns:
            æŸ¥è©¢çµæœ dict
        """
        params = {
            "query": query,
            "start": start.timestamp(),
            "end": end.timestamp(),
            "step": step
        }

        try:
            response = requests.get(self.query_url, params=params, timeout=30)
            response.raise_for_status()
            return response.json()
        except requests.exceptions.RequestException as e:
            print(f"âŒ æŸ¥è©¢å¤±æ•—: {query}", file=sys.stderr)
            print(f"   éŒ¯èª¤: {e}", file=sys.stderr)
            return {"status": "error", "data": {"result": []}}

    def export_throughput_metrics(self, start: datetime, end: datetime, output_file: str = "throughput_metrics.csv"):
        """
        åŒ¯å‡ºç³»çµ±ååé‡æŒ‡æ¨™åˆ° CSV

        é€™å€‹æ–¹æ³•æœƒæŸ¥è©¢ log-collection-dashboard.json ä¸­ã€Œç³»çµ±ååé‡ (Throughput)ã€
        é¢æ¿å®šç¾©çš„æ‰€æœ‰ 4 å€‹æŒ‡æ¨™ (ä½¿ç”¨ irate[5s] ç¬æ™‚å³°å€¼):
        - æ—¥èªŒæ•¸ (logs/s) - ç¬æ™‚å³°å€¼
        - Redis è¨Šæ¯ (msg/s) - ç¬æ™‚å³°å€¼
        - PG æ’å…¥ (rows/s) - ç¬æ™‚å³°å€¼
        - HTTP è«‹æ±‚ (req/s) - ç¬æ™‚å³°å€¼
        """

        # æ“´å±•æ™‚é–“ç¯„åœï¼šé–‹å§‹æ™‚é–“å¾€å‰æ¨ 1 åˆ†é˜ï¼ŒçµæŸæ™‚é–“å¾€å¾Œæ¨ 1 åˆ†é˜
        # é€™æ¨£å¯ä»¥ç¢ºä¿æ“·å–åˆ°å®Œæ•´çš„æ¸¬è©¦è³‡æ–™
        extended_start = start - timedelta(minutes=1)
        extended_end = end + timedelta(minutes=1)

        # å®šç¾©è¦æŸ¥è©¢çš„æŒ‡æ¨™ (æ ¸å¿ƒæ•ˆèƒ½æ¯”è¼ƒæŒ‡æ¨™)
        # ä¿®æ”¹èªªæ˜ï¼šçµ±ä¸€ä¸»ç³»çµ±èˆ‡å°ç…§çµ„çš„æŸ¥è©¢æŒ‡æ¨™ï¼Œç¢ºä¿ä¸€è‡´æ€§æ¯”è¼ƒ
        # ä¸‰å¤§æ ¸å¿ƒæŒ‡æ¨™ï¼š1ï¸âƒ£æ—¥èªŒååé‡ã€2ï¸âƒ£HTTP QPSã€3ï¸âƒ£PGæ’å…¥é€Ÿç‡
        queries = [
            {
                "name": "1_logs_throughput",
                "query": "sum(irate(logs_received_total[5s]))",
                "description": "1ï¸âƒ£ æ—¥èªŒååé‡ (logs/s)"
            },
            {
                "name": "2_http_qps",
                "query": "sum(irate(http_requests_total[5s]))",
                "description": "2ï¸âƒ£ HTTP QPS (req/s)"
            },
            {
                "name": "3_pg_insert_rate",
                "query": "sum(rate(pg_stat_database_tup_inserted{datname=\"logsdb\"}[30s]))",
                "description": "3ï¸âƒ£ PG æ’å…¥é€Ÿç‡ (rows/s)"
            },
            {
                "name": "redis_messages_per_second",
                "query": "sum(irate(redis_stream_messages_total{status='success'}[5s]))",
                "description": "Redis è¨Šæ¯ (msg/s) - ä¸»ç³»çµ±æ¶æ§‹ç‰¹æœ‰"
            }
        ]

        print(f"ğŸ“Š é–‹å§‹æŸ¥è©¢ååé‡æŒ‡æ¨™...")
        print(f"   åŸå§‹æ™‚é–“ç¯„åœ: {start} ~ {end}")
        print(f"   æ“´å±•æ™‚é–“ç¯„åœ: {extended_start} ~ {extended_end}")
        print(f"   (å‰å¾Œå„æ“´å±• 1 åˆ†é˜ä»¥ç¢ºä¿è³‡æ–™å®Œæ•´æ€§)")
        print(f"   æŸ¥è©¢æŒ‡æ¨™æ•¸: {len(queries)}")
        print()

        # æŸ¥è©¢æ‰€æœ‰æŒ‡æ¨™ï¼ˆä½¿ç”¨æ“´å±•å¾Œçš„æ™‚é–“ç¯„åœï¼‰
        all_data = {}
        timestamps = set()

        for metric in queries:
            print(f"   æŸ¥è©¢: {metric['description']}")
            result = self.query_range(
                metric['query'], extended_start, extended_end, step="1s"
            )

            if result.get("status") == "success" and result.get("data", {}).get("result"):
                # å–å¾—ç¬¬ä¸€å€‹çµæœ (å› ç‚ºä½¿ç”¨ sum() èšåˆ)
                values = result["data"]["result"][0].get("values", [])

                # å°‡è³‡æ–™å­˜å…¥ dictï¼Œä»¥ timestamp ç‚º key
                metric_data = {}
                for ts, value in values:
                    timestamp = datetime.fromtimestamp(ts)
                    timestamps.add(timestamp)
                    metric_data[timestamp] = float(value)

                all_data[metric['name']] = {
                    'description': metric['description'],
                    'data': metric_data
                }
                print(f"      âœ… å–å¾— {len(values)} ç­†è³‡æ–™")
            else:
                print(f"      âš ï¸  ç„¡è³‡æ–™æˆ–æŸ¥è©¢å¤±æ•—")
                all_data[metric['name']] = {
                    'description': metric['description'],
                    'data': {}
                }

        print()

        # å¦‚æœæ²’æœ‰ä»»ä½•è³‡æ–™ï¼Œæå‰çµæŸ
        if not timestamps:
            print("âŒ æ²’æœ‰ä»»ä½•è³‡æ–™å¯åŒ¯å‡º")
            print("   è«‹ç¢ºèª:")
            print("   1. Prometheus æœå‹™æ˜¯å¦æ­£åœ¨é‹è¡Œ (http://localhost:9090)")
            print("   2. æ™‚é–“ç¯„åœå…§æ˜¯å¦æœ‰è³‡æ–™")
            print("   3. æŒ‡æ¨™åç¨±æ˜¯å¦æ­£ç¢º")
            return

        # æ’åºæ™‚é–“æˆ³è¨˜
        sorted_timestamps = sorted(timestamps)

        # æº–å‚™è¼¸å‡ºè·¯å¾‘ - åŸå§‹è³‡æ–™æª”æ¡ˆ
        # ä½¿ç”¨çµ•å°è·¯å¾‘ä¾†ç¢ºä¿æ­£ç¢ºæ‰¾åˆ°å°ˆæ¡ˆæ ¹ç›®éŒ„
        script_dir = Path(__file__).resolve().parent  # monitoring/scripts directory
        project_root = script_dir.parent.parent  # log-collection-system directory
        test_file_dir = project_root / "test_file"
        test_file_dir.mkdir(parents=True, exist_ok=True)

        original_output_file = str(test_file_dir / "monitoring_throughput_metrics.csv")

        # å¯«å…¥åŸå§‹ CSV
        print(f"ğŸ’¾ å¯«å…¥åŸå§‹ CSV: {original_output_file}")
        with open(original_output_file, 'w', newline='', encoding='utf-8-sig') as csvfile:
            # æº–å‚™æ¬„ä½åç¨±
            fieldnames = ['timestamp'] + [
                f"{metric['name']} ({metric['description']})"
                for metric in queries
            ]

            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()

            # å¯«å…¥è³‡æ–™
            for ts in sorted_timestamps:
                row = {'timestamp': ts.strftime('%Y-%m-%d %H:%M:%S')}

                for metric in queries:
                    metric_name = metric['name']
                    column_name = f"{metric_name} ({metric['description']})"

                    # å–å¾—è©²æ™‚é–“é»çš„å€¼ï¼Œå¦‚æœæ²’æœ‰å‰‡ç•™ç©º
                    value = all_data[metric_name]['data'].get(ts, '')
                    row[column_name] = value

                writer.writerow(row)

        print(f"âœ… åŸå§‹è³‡æ–™åŒ¯å‡ºå®Œæˆ!")
        print(f"   æª”æ¡ˆ: {original_output_file}")
        print(f"   è³‡æ–™ç­†æ•¸: {len(sorted_timestamps)}")
        print(f"   æ™‚é–“ç¯„åœ: {sorted_timestamps[0]} ~ {sorted_timestamps[-1]}")
        print()

        # ç¯©é¸ä¸¦åŒ¯å‡º HTTP QPS Top 20
        http_qps_data = all_data.get('2_http_qps', {}).get('data', {})
        if http_qps_data:
            print()
            print("ğŸ” é–‹å§‹é€²è¡Œ HTTP QPS Top 20 ç¯©é¸...")

            # æ’é™¤é›¶å€¼ã€ç©ºå€¼å’Œnullå€¼ï¼Œä¸¦æŒ‰ HTTP QPS é™åºæ’åº
            # å»ºç«‹ (timestamp, http_qps_value) çš„åˆ—è¡¨
            valid_data = [
                (ts, v) for ts, v in http_qps_data.items()
                if v is not None and v != '' and v != 0
            ]

            if not valid_data:
                print("   âš ï¸  æ‰€æœ‰ 2_http_qps è³‡æ–™éƒ½æ˜¯é›¶å€¼/ç©ºå€¼/nullï¼Œç„¡æ³•é€²è¡Œç¯©é¸")
                return

            # ä½¿ç”¨ pandas æ’åºä¸¦å–å‰ 20 ç­†
            df_temp = pd.DataFrame(valid_data, columns=['timestamp', 'http_qps'])
            df_sorted = df_temp.sort_values(by='http_qps', ascending=False)
            df_top20 = df_sorted.head(20)

            print(f"   åŸå§‹è³‡æ–™ç­†æ•¸: {len(http_qps_data)}")
            print(f"   éé›¶è³‡æ–™ç­†æ•¸: {len(valid_data)}")
            print(f"   ç¯©é¸å¾Œç­†æ•¸: {len(df_top20)}")
            print(f"   HTTP QPS ç¯„åœ: {df_top20['http_qps'].min():.2f} ~ {df_top20['http_qps'].max():.2f}")

            # å–å¾—å‰ 20 ç­†çš„æ™‚é–“æˆ³è¨˜
            filtered_timestamps = df_top20['timestamp'].tolist()
            # æŒ‰æ™‚é–“æ’åºï¼ˆæ–¹ä¾¿é–±è®€ï¼‰
            filtered_timestamps.sort()

            # åŒ¯å‡ºç¯©é¸å¾Œçš„è³‡æ–™åˆ°å›ºå®šæª”å
            filtered_output_file = str(test_file_dir / "monitoring_throughput_http_qps_top20.csv")
            print()
            print(f"ğŸ’¾ åŒ¯å‡ºç¯©é¸å¾Œ Top 20 è³‡æ–™: {filtered_output_file}")

            with open(filtered_output_file, 'w', newline='', encoding='utf-8-sig') as csvfile:
                # æº–å‚™æ¬„ä½åç¨± (èˆ‡åŸå§‹æª”æ¡ˆç›¸åŒ)
                fieldnames = ['timestamp'] + [
                    f"{metric['name']} ({metric['description']})"
                    for metric in queries
                ]

                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writeheader()

                # å¯«å…¥ç¯©é¸å¾Œçš„è³‡æ–™
                for ts in filtered_timestamps:
                    row = {'timestamp': ts.strftime('%Y-%m-%d %H:%M:%S')}

                    for metric in queries:
                        metric_name = metric['name']
                        column_name = f"{metric_name} ({metric['description']})"

                        # å–å¾—è©²æ™‚é–“é»çš„å€¼ï¼Œå¦‚æœæ²’æœ‰å‰‡ç•™ç©º
                        value = all_data[metric_name]['data'].get(ts, '')
                        row[column_name] = value

                    writer.writerow(row)

            print(f"âœ… Top 20 è³‡æ–™åŒ¯å‡ºå®Œæˆ!")
            print(f"   æª”æ¡ˆ: {filtered_output_file}")
            print(f"   è³‡æ–™ç­†æ•¸: {len(filtered_timestamps)}")
            if filtered_timestamps:
                print(f"   æ™‚é–“ç¯„åœ: {filtered_timestamps[0]} ~ {filtered_timestamps[-1]}")

            # é¡¯ç¤ºç¯©é¸å¾Œçš„çµ±è¨ˆæ‘˜è¦
            print()
            print("ğŸ“Š Top 20 çµ±è¨ˆæ‘˜è¦:")
            for metric in queries:
                metric_name = metric['name']
                # åªå–ç¯©é¸å¾Œæ™‚é–“æˆ³è¨˜çš„è³‡æ–™
                filtered_metric_values = [
                    all_data[metric_name]['data'].get(ts, 0)
                    for ts in filtered_timestamps
                    if all_data[metric_name]['data'].get(ts) is not None
                ]

                if filtered_metric_values:
                    print(f"   {metric['description']}:")
                    print(f"      å¹³å‡å€¼: {sum(filtered_metric_values)/len(filtered_metric_values):.2f}")
                    print(f"      æœ€å¤§å€¼: {max(filtered_metric_values):.2f}")
                    print(f"      æœ€å°å€¼: {min(filtered_metric_values):.2f}")

        else:
            print()
            print("âš ï¸  ç„¡æ³•é€²è¡Œç¯©é¸: 2_http_qps è³‡æ–™ä¸å­˜åœ¨")

    def filter_http_qps_top20(self, csv_file: str) -> str:
        """
        æ–°å¢åŠŸèƒ½ï¼šç¯©é¸ HTTP QPS æ¬„ä½ï¼Œæ’é™¤ 0ã€ç©ºå€¼å’Œ nullï¼Œ
        æŒ‰ç…§é™åºæ’åºï¼Œå–å‰ 20 ç­†ï¼ŒåŒ¯å‡ºåˆ°å›ºå®šæª”åçš„ CSV

        Args:
            csv_file: è¼¸å…¥çš„ CSV æª”æ¡ˆè·¯å¾‘

        Returns:
            è¼¸å‡ºæª”æ¡ˆè·¯å¾‘
        """
        print()
        print("=" * 70)
        print("  ğŸ” HTTP QPS Top 20 åˆ†æ")
        print("=" * 70)
        print(f"   è®€å–æª”æ¡ˆ: {csv_file}")

        try:
            # è®€å– CSV
            df = pd.read_csv(csv_file)
            print(f"   åŸå§‹è³‡æ–™ç­†æ•¸: {len(df)}")

            # å°‹æ‰¾ HTTP QPS æ¬„ä½
            http_qps_column = None
            for col in df.columns:
                if '2_http_qps' in col:
                    http_qps_column = col
                    break

            if http_qps_column is None:
                print("âŒ æ‰¾ä¸åˆ° '2_http_qps' æ¬„ä½")
                print(f"   å¯ç”¨æ¬„ä½: {list(df.columns)}")
                return None

            print(f"   ç›®æ¨™æ¬„ä½: '{http_qps_column}'")

            # ç¯©é¸æ‰ 0ã€ç©ºå€¼å’Œ null
            df_clean = df.copy()
            df_clean = df_clean[pd.notna(df_clean[http_qps_column]) & (df_clean[http_qps_column] != '')]
            df_clean.loc[:, http_qps_column] = pd.to_numeric(df_clean[http_qps_column], errors='coerce')
            df_clean = df_clean.dropna(subset=[http_qps_column])
            df_clean = df_clean[df_clean[http_qps_column] > 0]

            print(f"   ç¯©é¸å¾Œè³‡æ–™ç­†æ•¸: {len(df_clean)} (ç§»é™¤äº† {len(df) - len(df_clean)} ç­†ç„¡æ•ˆè³‡æ–™)")

            if len(df_clean) == 0:
                print("âŒ ç¯©é¸å¾Œæ²’æœ‰æœ‰æ•ˆè³‡æ–™")
                return None

            # é™åºæ’åºä¸¦å–å‰ 20 ç­†
            df_sorted = df_clean.sort_values(by=http_qps_column, ascending=False)
            df_top20 = df_sorted.head(20)

            print(f"   å–å¾—å‰ 20 ç­†è³‡æ–™")

            # ç¢ºå®šè¼¸å‡ºæª”æ¡ˆè·¯å¾‘ï¼ˆå›ºå®šæª”åï¼Œæ”¾åœ¨ test_file/ ç›®éŒ„ï¼‰
            from pathlib import Path
            # ä½¿ç”¨çµ•å°è·¯å¾‘ä¾†ç¢ºä¿æ­£ç¢ºæ‰¾åˆ°å°ˆæ¡ˆæ ¹ç›®éŒ„
            script_dir = Path(__file__).resolve().parent  # monitoring/scripts directory
            project_root = script_dir.parent.parent  # log-collection-system directory
            test_file_dir = project_root / "test_file"
            test_file_dir.mkdir(parents=True, exist_ok=True)

            output_file = str(test_file_dir / "http_qps_top20.csv")

            # åŒ¯å‡º CSV
            df_top20.to_csv(output_file, index=False, encoding='utf-8-sig')

            print()
            print(f"âœ… åŒ¯å‡ºå®Œæˆ!")
            print(f"   è¼¸å‡ºæª”æ¡ˆ: {output_file}")
            print(f"   è³‡æ–™ç­†æ•¸: {len(df_top20)}")
            print()
            print("ğŸ“Š Top 20 çµ±è¨ˆæ‘˜è¦:")
            print(f"   æœ€å¤§å€¼: {df_top20[http_qps_column].max():.2f}")
            print(f"   æœ€å°å€¼: {df_top20[http_qps_column].min():.2f}")
            print(f"   å¹³å‡å€¼: {df_top20[http_qps_column].mean():.2f}")
            print(f"   ä¸­ä½æ•¸: {df_top20[http_qps_column].median():.2f}")
            print("=" * 70)

            return output_file

        except Exception as e:
            print(f"âŒ è™•ç†å¤±æ•—: {e}")
            import traceback
            traceback.print_exc()
            return None


def parse_duration(duration_str: str) -> timedelta:
    """
    è§£ææ™‚é–“é•·åº¦å­—ä¸²

    Args:
        duration_str: æ™‚é–“å­—ä¸²ï¼Œå¦‚ "1h", "30m", "2d"

    Returns:
        timedelta ç‰©ä»¶
    """
    unit = duration_str[-1]
    value = int(duration_str[:-1])

    if unit == 's':
        return timedelta(seconds=value)
    elif unit == 'm':
        return timedelta(minutes=value)
    elif unit == 'h':
        return timedelta(hours=value)
    elif unit == 'd':
        return timedelta(days=value)
    else:
        raise ValueError(f"ä¸æ”¯æ´çš„æ™‚é–“å–®ä½: {unit} (æ”¯æ´: s, m, h, d)")


def main():
    parser = argparse.ArgumentParser(
        description='åŒ¯å‡º Prometheus ç³»çµ±ååé‡æŒ‡æ¨™åˆ° CSV',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¯„ä¾‹:
  # åŒ¯å‡ºæœ€è¿‘ 1 å°æ™‚çš„è³‡æ–™
  %(prog)s --duration 1h

  # åŒ¯å‡ºæœ€è¿‘ 30 åˆ†é˜çš„è³‡æ–™ï¼ŒæŒ‡å®šè¼¸å‡ºæª”å
  %(prog)s --duration 30m --output my_metrics.csv

  # åŒ¯å‡ºæŒ‡å®šæ™‚é–“ç¯„åœçš„è³‡æ–™
  %(prog)s --start "2024-11-25T10:00:00" --end "2024-11-25T11:00:00"

  # æŒ‡å®š Prometheus URL
  %(prog)s --duration 1h --prometheus http://prometheus:9090
        """
    )

    parser.add_argument(
        '--prometheus',
        default=PROMETHEUS_URL,
        help=f'Prometheus URL (é è¨­: {PROMETHEUS_URL})'
    )

    parser.add_argument(
        '--duration',
        help='æŸ¥è©¢æ™‚é–“é•·åº¦ (ä¾‹: 1h, 30m, 2d)ã€‚æœƒå¾ç¾åœ¨å¾€å‰æ¨ç®—ã€‚'
    )

    parser.add_argument(
        '--start',
        help='é–‹å§‹æ™‚é–“ (ISO æ ¼å¼ï¼Œä¾‹: 2024-11-25T10:00:00)'
    )

    parser.add_argument(
        '--end',
        help='çµæŸæ™‚é–“ (ISO æ ¼å¼ï¼Œä¾‹: 2024-11-25T11:00:00)'
    )

    parser.add_argument(
        '--output', '-o',
        default='throughput_metrics.csv',
        help='è¼¸å‡º CSV æª”æ¡ˆåç¨± (é è¨­: throughput_metrics.csv)'
    )

    args = parser.parse_args()

    # æ±ºå®šæ™‚é–“ç¯„åœ
    if args.duration:
        # ä½¿ç”¨ç›¸å°æ™‚é–“
        end_time = datetime.now()
        duration = parse_duration(args.duration)
        start_time = end_time - duration
    elif args.start and args.end:
        # ä½¿ç”¨çµ•å°æ™‚é–“
        start_time = datetime.fromisoformat(args.start.replace('Z', '+00:00'))
        end_time = datetime.fromisoformat(args.end.replace('Z', '+00:00'))
    else:
        # é è¨­: æœ€è¿‘ 1 å°æ™‚
        print("âš ï¸  æœªæŒ‡å®šæ™‚é–“ç¯„åœï¼Œä½¿ç”¨é è¨­å€¼: æœ€è¿‘ 1 å°æ™‚")
        print()
        end_time = datetime.now()
        start_time = end_time - timedelta(hours=1)

    # å»ºç«‹ exporter ä¸¦åŸ·è¡ŒåŒ¯å‡º
    exporter = PrometheusExporter(args.prometheus)
    exporter.export_throughput_metrics(start_time, end_time, args.output)

    # ä¿®æ”¹ï¼šç§»é™¤è‡ªå‹•åŸ·è¡Œ HTTP QPS Top 20 åˆ†æï¼ˆå·²æ”¹ç‚ºç›´æ¥åœ¨ export_throughput_metrics ä¸­é€²è¡Œç¯©é¸ä¸¦è¦†è“‹åŸæª”æ¡ˆï¼‰
    # åŸç¨‹å¼ç¢¼ï¼ˆå·²è¨»é‡‹ï¼‰ï¼š
    # if os.path.exists(args.output):
    #     try:
    #         exporter.filter_http_qps_top20(args.output)
    #     except Exception as e:
    #         print(f"\nâš ï¸  HTTP QPS Top 20 åˆ†æå¤±æ•—: {e}")
    #         print("   ä¸»è¦åŒ¯å‡ºæª”æ¡ˆä¸å—å½±éŸ¿")


if __name__ == "__main__":
    main()
