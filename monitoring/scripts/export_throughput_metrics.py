#!/usr/bin/env python3
"""
ç³»çµ±ååé‡æŒ‡æ¨™åŒ¯å‡ºå·¥å…·

æ­¤è…³æœ¬æœƒæŸ¥è©¢ Prometheus ä¸­ã€Œç³»çµ±ååé‡ (Throughput)ã€åœ–è¡¨çš„æ‰€æœ‰æŒ‡æ¨™è³‡æ–™ï¼Œ
ä¸¦å°‡çµæœåŒ¯å‡ºç‚º CSV æª”æ¡ˆã€‚

ä½¿ç”¨æ–¹å¼:
    python export_throughput_metrics.py --start "2024-11-25T00:00:00Z" --end "2024-11-25T23:59:59Z"
    æˆ–
    python export_throughput_metrics.py --duration 1h  # æœ€è¿‘ 1 å°æ™‚
    python export_throughput_metrics.py --duration 30m # æœ€è¿‘ 30 åˆ†é˜
"""

import argparse
import csv
import sys
from datetime import datetime, timedelta
from typing import List, Dict, Any
import requests
from urllib.parse import urljoin
import pandas as pd  # æ–°å¢: ç”¨æ–¼è¨ˆç®—ä¸­ä½æ•¸å’Œè³‡æ–™ç¯©é¸

# Prometheus é€£ç·šè¨­å®š
PROMETHEUS_URL = "http://localhost:9090"


class PrometheusExporter:
    """Prometheus æŒ‡æ¨™æŸ¥è©¢èˆ‡åŒ¯å‡ºå·¥å…·"""

    def __init__(self, prometheus_url: str = PROMETHEUS_URL):
        self.prometheus_url = prometheus_url
        self.query_url = urljoin(prometheus_url, "/api/v1/query_range")

    def query_range(self, query: str, start: datetime, end: datetime, step: str = "1s") -> Dict[str, Any]:
        """
        æŸ¥è©¢ Prometheus æ™‚é–“ç¯„åœè³‡æ–™

        Args:
            query: PromQL æŸ¥è©¢èªå¥
            start: é–‹å§‹æ™‚é–“
            end: çµæŸæ™‚é–“
            step: æ™‚é–“é–“éš” (é è¨­ 1s)

        Returns:
            æŸ¥è©¢çµæœ dict
        """
        params = {
            "query": query,
            "start": start.timestamp(),
            "end": end.timestamp(),
            "step": step
        }

        try:
            response = requests.get(self.query_url, params=params, timeout=30)
            response.raise_for_status()
            return response.json()
        except requests.exceptions.RequestException as e:
            print(f"âŒ æŸ¥è©¢å¤±æ•—: {query}", file=sys.stderr)
            print(f"   éŒ¯èª¤: {e}", file=sys.stderr)
            return {"status": "error", "data": {"result": []}}

    def export_throughput_metrics(self, start: datetime, end: datetime, output_file: str = "throughput_metrics.csv"):
        """
        åŒ¯å‡ºç³»çµ±ååé‡æŒ‡æ¨™åˆ° CSV

        é€™å€‹æ–¹æ³•æœƒæŸ¥è©¢ log-collection-dashboard.json ä¸­ã€Œç³»çµ±ååé‡ (Throughput)ã€
        é¢æ¿å®šç¾©çš„æ‰€æœ‰ 4 å€‹æŒ‡æ¨™ (ä½¿ç”¨ irate[5s] ç¬æ™‚å³°å€¼):
        - æ—¥èªŒæ•¸ (logs/s) - ç¬æ™‚å³°å€¼
        - Redis è¨Šæ¯ (msg/s) - ç¬æ™‚å³°å€¼
        - PG æ’å…¥ (rows/s) - ç¬æ™‚å³°å€¼
        - HTTP è«‹æ±‚ (req/s) - ç¬æ™‚å³°å€¼
        """

        # æ“´å±•æ™‚é–“ç¯„åœï¼šé–‹å§‹æ™‚é–“å¾€å‰æ¨ 1 åˆ†é˜ï¼ŒçµæŸæ™‚é–“å¾€å¾Œæ¨ 1 åˆ†é˜
        # é€™æ¨£å¯ä»¥ç¢ºä¿æ“·å–åˆ°å®Œæ•´çš„æ¸¬è©¦è³‡æ–™
        extended_start = start - timedelta(minutes=1)
        extended_end = end + timedelta(minutes=1)

        # å®šç¾©è¦æŸ¥è©¢çš„æŒ‡æ¨™ (ä¾†è‡ª dashboard panel id=0)
        # ä¿®æ”¹èªªæ˜ï¼šä½¿ç”¨èˆ‡ dashboard ä¸€è‡´çš„ irate[5s] ç¬æ™‚å³°å€¼æŸ¥è©¢
        # åŸå§‹æŸ¥è©¢ä½¿ç”¨ [30s] å’Œ [1m]ï¼Œç¾å·²èª¿æ•´ç‚º [5s] ä»¥ç¬¦åˆ dashboard è¨­å®š
        # æ–°å¢: logs_per_second_30s ç”¨æ–¼ä¸­ä½æ•¸ç¯©é¸åˆ†æ
        queries = [
            {
                "name": "logs_per_second",
                "query": "sum(irate(logs_received_total[5s]))",
                "description": "æ—¥èªŒæ•¸ (logs/s) - ç¬æ™‚å³°å€¼"
            },
            {
                "name": "logs_per_second_30s",
                "query": "sum(rate(logs_received_total[30s]))",
                "description": "æ—¥èªŒæ•¸ (logs/s) - 30ç§’å¹³å‡",
                "filter_by_median": True  # æ¨™è¨˜æ­¤æ¬„ä½éœ€è¦é€²è¡Œä¸­ä½æ•¸ç¯©é¸
            },
            {
                "name": "redis_messages_per_second",
                "query": "sum(irate(redis_stream_messages_total{status='success'}[5s]))",
                "description": "Redis è¨Šæ¯ (msg/s) - ç¬æ™‚å³°å€¼"
            },
            {
                "name": "pg_inserts_per_second",
                "query": "sum(irate(pg_stat_database_tup_inserted{datname=\"logsdb\"}[5s]))",
                "description": "PG æ’å…¥ (rows/s) - ç¬æ™‚å³°å€¼"
            },
            {
                "name": "http_requests_per_second",
                "query": "sum(irate(http_requests_total[5s]))",
                "description": "HTTP è«‹æ±‚ (req/s) - ç¬æ™‚å³°å€¼"
            }
        ]

        print(f"ğŸ“Š é–‹å§‹æŸ¥è©¢ååé‡æŒ‡æ¨™...")
        print(f"   åŸå§‹æ™‚é–“ç¯„åœ: {start} ~ {end}")
        print(f"   æ“´å±•æ™‚é–“ç¯„åœ: {extended_start} ~ {extended_end}")
        print(f"   (å‰å¾Œå„æ“´å±• 1 åˆ†é˜ä»¥ç¢ºä¿è³‡æ–™å®Œæ•´æ€§)")
        print(f"   æŸ¥è©¢æŒ‡æ¨™æ•¸: {len(queries)}")
        print()

        # æŸ¥è©¢æ‰€æœ‰æŒ‡æ¨™ï¼ˆä½¿ç”¨æ“´å±•å¾Œçš„æ™‚é–“ç¯„åœï¼‰
        all_data = {}
        timestamps = set()

        for metric in queries:
            print(f"   æŸ¥è©¢: {metric['description']}")
            result = self.query_range(
                metric['query'], extended_start, extended_end, step="1s"
            )

            if result.get("status") == "success" and result.get("data", {}).get("result"):
                # å–å¾—ç¬¬ä¸€å€‹çµæœ (å› ç‚ºä½¿ç”¨ sum() èšåˆ)
                values = result["data"]["result"][0].get("values", [])

                # å°‡è³‡æ–™å­˜å…¥ dictï¼Œä»¥ timestamp ç‚º key
                metric_data = {}
                for ts, value in values:
                    timestamp = datetime.fromtimestamp(ts)
                    timestamps.add(timestamp)
                    metric_data[timestamp] = float(value)

                all_data[metric['name']] = {
                    'description': metric['description'],
                    'data': metric_data
                }
                print(f"      âœ… å–å¾— {len(values)} ç­†è³‡æ–™")
            else:
                print(f"      âš ï¸  ç„¡è³‡æ–™æˆ–æŸ¥è©¢å¤±æ•—")
                all_data[metric['name']] = {
                    'description': metric['description'],
                    'data': {}
                }

        print()

        # å¦‚æœæ²’æœ‰ä»»ä½•è³‡æ–™ï¼Œæå‰çµæŸ
        if not timestamps:
            print("âŒ æ²’æœ‰ä»»ä½•è³‡æ–™å¯åŒ¯å‡º")
            print("   è«‹ç¢ºèª:")
            print("   1. Prometheus æœå‹™æ˜¯å¦æ­£åœ¨é‹è¡Œ (http://localhost:9090)")
            print("   2. æ™‚é–“ç¯„åœå…§æ˜¯å¦æœ‰è³‡æ–™")
            print("   3. æŒ‡æ¨™åç¨±æ˜¯å¦æ­£ç¢º")
            return

        # æ’åºæ™‚é–“æˆ³è¨˜
        sorted_timestamps = sorted(timestamps)

        # å¯«å…¥ CSV
        print(f"ğŸ’¾ å¯«å…¥ CSV: {output_file}")
        with open(output_file, 'w', newline='', encoding='utf-8-sig') as csvfile:
            # æº–å‚™æ¬„ä½åç¨±
            fieldnames = ['timestamp'] + [
                f"{metric['name']} ({metric['description']})"
                for metric in queries
            ]

            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()

            # å¯«å…¥è³‡æ–™
            for ts in sorted_timestamps:
                row = {'timestamp': ts.strftime('%Y-%m-%d %H:%M:%S')}

                for metric in queries:
                    metric_name = metric['name']
                    column_name = f"{metric_name} ({metric['description']})"

                    # å–å¾—è©²æ™‚é–“é»çš„å€¼ï¼Œå¦‚æœæ²’æœ‰å‰‡ç•™ç©º
                    value = all_data[metric_name]['data'].get(ts, '')
                    row[column_name] = value

                writer.writerow(row)

        print(f"âœ… åŒ¯å‡ºå®Œæˆ!")
        print(f"   æª”æ¡ˆ: {output_file}")
        print(f"   è³‡æ–™ç­†æ•¸: {len(sorted_timestamps)}")
        print(f"   æ™‚é–“ç¯„åœ: {sorted_timestamps[0]} ~ {sorted_timestamps[-1]}")
        print()
        print("ğŸ“ˆ çµ±è¨ˆæ‘˜è¦:")
        for metric in queries:
            metric_name = metric['name']
            data_values = list(all_data[metric_name]['data'].values())
            if data_values:
                print(f"   {metric['description']}:")

                # æª¢æŸ¥æ˜¯å¦éœ€è¦é€²è¡Œä¸­ä½æ•¸ç¯©é¸
                if metric.get('filter_by_median', False):
                    # ä½¿ç”¨ pandas è¨ˆç®—ä¸­ä½æ•¸ä¸¦ç¯©é¸
                    df = pd.Series(data_values)
                    median_value = df.median()
                    filtered_values = df[df >= median_value]

                    print(f"      [åŸå§‹è³‡æ–™]")
                    print(f"      è³‡æ–™ç­†æ•¸: {len(data_values)}")
                    print(f"      æœ€å¤§å€¼: {max(data_values):.2f}")
                    print(f"      æœ€å°å€¼: {min(data_values):.2f}")
                    print(f"      å¹³å‡å€¼: {sum(data_values)/len(data_values):.2f}")
                    print(f"      ä¸­ä½æ•¸: {median_value:.2f}")
                    print(f"      ")
                    print(f"      [ç¯©é¸å¾Œè³‡æ–™ (>= ä¸­ä½æ•¸)]")
                    print(f"      ç¯©é¸å¾Œç­†æ•¸: {len(filtered_values)} ({len(filtered_values)/len(data_values)*100:.1f}%)")
                    print(f"      ç¯©é¸å¾Œå¹³å‡å€¼: {filtered_values.mean():.2f}")
                    print(f"      ç¯©é¸å¾Œæœ€å¤§å€¼: {filtered_values.max():.2f}")
                    print(f"      ç¯©é¸å¾Œæœ€å°å€¼: {filtered_values.min():.2f}")
                else:
                    # åŸå§‹çµ±è¨ˆï¼ˆä¸é€²è¡Œç¯©é¸ï¼‰
                    print(f"      æœ€å¤§å€¼: {max(data_values):.2f}")
                    print(f"      æœ€å°å€¼: {min(data_values):.2f}")
                    print(f"      å¹³å‡å€¼: {sum(data_values)/len(data_values):.2f}")

        # æ–°å¢åŠŸèƒ½: åŸºæ–¼ logs_per_second ä¸­ä½æ•¸ç¯©é¸è³‡æ–™ä¸¦åŒ¯å‡º
        # è¨ˆç®— logs_per_second çš„ä¸­ä½æ•¸ï¼ˆæ’é™¤é›¶å€¼ã€ç©ºå€¼ã€nullå€¼ï¼‰
        logs_data = all_data.get('logs_per_second', {}).get('data', {})
        if logs_data:
            print()
            print("ğŸ” é–‹å§‹é€²è¡Œä¸­ä½æ•¸ç¯©é¸...")

            # æ–¹æ¡ˆ2: æ’é™¤é›¶å€¼ã€ç©ºå€¼å’Œnullå€¼å¾Œå†è¨ˆç®—ä¸­ä½æ•¸
            # æ”¶é›†æ‰€æœ‰éé›¶ã€éç©ºã€énullçš„å€¼
            non_zero_logs_values = [
                v for v in logs_data.values()
                if v is not None and v != '' and v > 0
            ]

            if not non_zero_logs_values:
                print("   âš ï¸  æ‰€æœ‰ logs_per_second è³‡æ–™éƒ½æ˜¯é›¶å€¼/ç©ºå€¼/nullï¼Œç„¡æ³•é€²è¡Œç¯©é¸")
                print()
                print("âš ï¸  ç„¡æ³•é€²è¡Œä¸­ä½æ•¸ç¯©é¸: æ²’æœ‰æœ‰æ•ˆçš„ logs_per_second è³‡æ–™")
                return

            # ä½¿ç”¨ pandas è¨ˆç®—éé›¶å€¼çš„ä¸­ä½æ•¸
            logs_values_series = pd.Series(non_zero_logs_values)
            median_logs = logs_values_series.median()

            print(f"   åŸå§‹è³‡æ–™ç­†æ•¸: {len(logs_data)}")
            print(f"   éé›¶è³‡æ–™ç­†æ•¸: {len(non_zero_logs_values)} ({len(non_zero_logs_values)/len(logs_data)*100:.1f}%)")
            print(f"   éé›¶è³‡æ–™ä¸­ä½æ•¸: {median_logs:.2f}")
            print(f"   éé›¶è³‡æ–™å¹³å‡å€¼: {logs_values_series.mean():.2f}")
            print(f"   éé›¶è³‡æ–™æœ€å¤§å€¼: {logs_values_series.max():.2f}")
            print(f"   éé›¶è³‡æ–™æœ€å°å€¼: {logs_values_series.min():.2f}")

            # ç¯©é¸å‡º logs_per_second > ä¸­ä½æ•¸çš„æ™‚é–“æˆ³è¨˜ï¼ˆä½¿ç”¨ > è€Œé >=ï¼‰
            filtered_timestamps = [
                ts for ts in sorted_timestamps
                if logs_data.get(ts) is not None and logs_data.get(ts) != '' and logs_data.get(ts) > median_logs
            ]

            print(f"   ç¯©é¸æ¢ä»¶: logs_per_second > {median_logs:.2f}")
            print(f"   ç¯©é¸å¾Œç­†æ•¸: {len(filtered_timestamps)} ({len(filtered_timestamps)/len(sorted_timestamps)*100:.1f}%)")

            # åŒ¯å‡ºç¯©é¸å¾Œçš„è³‡æ–™
            filtered_output = output_file.replace('.csv', '_filtered.csv')
            print()
            print(f"ğŸ’¾ å¯«å…¥ç¯©é¸å¾Œçš„ CSV: {filtered_output}")

            with open(filtered_output, 'w', newline='', encoding='utf-8-sig') as csvfile:
                # æº–å‚™æ¬„ä½åç¨± (èˆ‡åŸå§‹æª”æ¡ˆç›¸åŒ)
                fieldnames = ['timestamp'] + [
                    f"{metric['name']} ({metric['description']})"
                    for metric in queries
                ]

                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writeheader()

                # å¯«å…¥ç¯©é¸å¾Œçš„è³‡æ–™
                for ts in filtered_timestamps:
                    row = {'timestamp': ts.strftime('%Y-%m-%d %H:%M:%S')}

                    for metric in queries:
                        metric_name = metric['name']
                        column_name = f"{metric_name} ({metric['description']})"

                        # å–å¾—è©²æ™‚é–“é»çš„å€¼ï¼Œå¦‚æœæ²’æœ‰å‰‡ç•™ç©º
                        value = all_data[metric_name]['data'].get(ts, '')
                        row[column_name] = value

                    writer.writerow(row)

            print(f"âœ… ç¯©é¸å¾Œè³‡æ–™åŒ¯å‡ºå®Œæˆ!")
            print(f"   æª”æ¡ˆ: {filtered_output}")
            print(f"   è³‡æ–™ç­†æ•¸: {len(filtered_timestamps)}")
            if filtered_timestamps:
                print(f"   æ™‚é–“ç¯„åœ: {filtered_timestamps[0]} ~ {filtered_timestamps[-1]}")

            # é¡¯ç¤ºç¯©é¸å¾Œçš„çµ±è¨ˆæ‘˜è¦
            print()
            print("ğŸ“Š ç¯©é¸å¾Œçµ±è¨ˆæ‘˜è¦:")
            for metric in queries:
                metric_name = metric['name']
                # åªå–ç¯©é¸å¾Œæ™‚é–“æˆ³è¨˜çš„è³‡æ–™
                filtered_metric_values = [
                    all_data[metric_name]['data'].get(ts, 0)
                    for ts in filtered_timestamps
                    if all_data[metric_name]['data'].get(ts) is not None
                ]

                if filtered_metric_values:
                    print(f"   {metric['description']}:")
                    print(f"      å¹³å‡å€¼: {sum(filtered_metric_values)/len(filtered_metric_values):.2f}")
                    print(f"      æœ€å¤§å€¼: {max(filtered_metric_values):.2f}")
                    print(f"      æœ€å°å€¼: {min(filtered_metric_values):.2f}")
        else:
            print()
            print("âš ï¸  ç„¡æ³•é€²è¡Œä¸­ä½æ•¸ç¯©é¸: logs_per_second è³‡æ–™ä¸å­˜åœ¨")


def parse_duration(duration_str: str) -> timedelta:
    """
    è§£ææ™‚é–“é•·åº¦å­—ä¸²

    Args:
        duration_str: æ™‚é–“å­—ä¸²ï¼Œå¦‚ "1h", "30m", "2d"

    Returns:
        timedelta ç‰©ä»¶
    """
    unit = duration_str[-1]
    value = int(duration_str[:-1])

    if unit == 's':
        return timedelta(seconds=value)
    elif unit == 'm':
        return timedelta(minutes=value)
    elif unit == 'h':
        return timedelta(hours=value)
    elif unit == 'd':
        return timedelta(days=value)
    else:
        raise ValueError(f"ä¸æ”¯æ´çš„æ™‚é–“å–®ä½: {unit} (æ”¯æ´: s, m, h, d)")


def main():
    parser = argparse.ArgumentParser(
        description='åŒ¯å‡º Prometheus ç³»çµ±ååé‡æŒ‡æ¨™åˆ° CSV',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¯„ä¾‹:
  # åŒ¯å‡ºæœ€è¿‘ 1 å°æ™‚çš„è³‡æ–™
  %(prog)s --duration 1h

  # åŒ¯å‡ºæœ€è¿‘ 30 åˆ†é˜çš„è³‡æ–™ï¼ŒæŒ‡å®šè¼¸å‡ºæª”å
  %(prog)s --duration 30m --output my_metrics.csv

  # åŒ¯å‡ºæŒ‡å®šæ™‚é–“ç¯„åœçš„è³‡æ–™
  %(prog)s --start "2024-11-25T10:00:00" --end "2024-11-25T11:00:00"

  # æŒ‡å®š Prometheus URL
  %(prog)s --duration 1h --prometheus http://prometheus:9090
        """
    )

    parser.add_argument(
        '--prometheus',
        default=PROMETHEUS_URL,
        help=f'Prometheus URL (é è¨­: {PROMETHEUS_URL})'
    )

    parser.add_argument(
        '--duration',
        help='æŸ¥è©¢æ™‚é–“é•·åº¦ (ä¾‹: 1h, 30m, 2d)ã€‚æœƒå¾ç¾åœ¨å¾€å‰æ¨ç®—ã€‚'
    )

    parser.add_argument(
        '--start',
        help='é–‹å§‹æ™‚é–“ (ISO æ ¼å¼ï¼Œä¾‹: 2024-11-25T10:00:00)'
    )

    parser.add_argument(
        '--end',
        help='çµæŸæ™‚é–“ (ISO æ ¼å¼ï¼Œä¾‹: 2024-11-25T11:00:00)'
    )

    parser.add_argument(
        '--output', '-o',
        default='throughput_metrics.csv',
        help='è¼¸å‡º CSV æª”æ¡ˆåç¨± (é è¨­: throughput_metrics.csv)'
    )

    args = parser.parse_args()

    # æ±ºå®šæ™‚é–“ç¯„åœ
    if args.duration:
        # ä½¿ç”¨ç›¸å°æ™‚é–“
        end_time = datetime.now()
        duration = parse_duration(args.duration)
        start_time = end_time - duration
    elif args.start and args.end:
        # ä½¿ç”¨çµ•å°æ™‚é–“
        start_time = datetime.fromisoformat(args.start.replace('Z', '+00:00'))
        end_time = datetime.fromisoformat(args.end.replace('Z', '+00:00'))
    else:
        # é è¨­: æœ€è¿‘ 1 å°æ™‚
        print("âš ï¸  æœªæŒ‡å®šæ™‚é–“ç¯„åœï¼Œä½¿ç”¨é è¨­å€¼: æœ€è¿‘ 1 å°æ™‚")
        print()
        end_time = datetime.now()
        start_time = end_time - timedelta(hours=1)

    # å»ºç«‹ exporter ä¸¦åŸ·è¡ŒåŒ¯å‡º
    exporter = PrometheusExporter(args.prometheus)
    exporter.export_throughput_metrics(start_time, end_time, args.output)


if __name__ == "__main__":
    main()
