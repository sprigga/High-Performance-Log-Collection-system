"""
å°ç…§çµ„å£“åŠ›æ¸¬è©¦è…³æœ¬ - æ¸¬è©¦ç°¡åŒ–ç‰ˆç³»çµ±
ç›´æ¥å¯«å…¥ PostgreSQLï¼Œç„¡è² è¼‰å¹³è¡¡ã€é€£æ¥æ± ã€Redisã€Worker

æ•´åˆåŠŸèƒ½ï¼š
- å£“åŠ›æ¸¬è©¦åŸ·è¡Œ
- Prometheus æŒ‡æ¨™è‡ªå‹•æ“·å–èˆ‡åŒ¯å‡º
"""
import asyncio
import aiohttp
import time
import random
import csv
import requests
import os
from datetime import datetime, timedelta
from typing import List, Dict, Any
from urllib.parse import urljoin
from pathlib import Path

# ==========================================
# æ¸¬è©¦é…ç½®
# ==========================================
BASE_URL = "http://localhost:18724"  # å°ç…§çµ„ç«¯é»
NUM_DEVICES = 100                    # è¨­å‚™æ•¸é‡
LOGS_PER_DEVICE = 100                # æ¯å°è¨­å‚™ç™¼é€çš„æ—¥èªŒæ•¸
CONCURRENT_LIMIT = 200               # ä¸¦ç™¼é™åˆ¶
BATCH_SIZE = 5                       # æ‰¹æ¬¡å¤§å°
USE_BATCH_API = True                 # æ˜¯å¦ä½¿ç”¨æ‰¹é‡ API
NUM_ITERATIONS = 20                 # æ¸¬è©¦åŸ·è¡Œçš„å¾ªç’°æ¬¡æ•¸
ITERATION_INTERVAL = 5               # æ¯æ¬¡å¾ªç’°ä¹‹é–“çš„é–“éš”æ™‚é–“ï¼ˆç§’ï¼‰

# Prometheus ç›£æ§é…ç½®
PROMETHEUS_URL = "http://localhost:19090"  # å°ç…§çµ„ Prometheus ç«¯é»
EXPORT_METRICS = True                # æ˜¯å¦è‡ªå‹•åŒ¯å‡ºæŒ‡æ¨™

# ä¿®æ”¹ï¼šä½¿ç”¨ç›¸å°è·¯å¾‘ï¼Œå‹•æ…‹è¨ˆç®—å°ˆæ¡ˆæ ¹ç›®éŒ„ä¸‹çš„ test_file/ ç›®éŒ„
# å–å¾—è…³æœ¬æ‰€åœ¨ç›®éŒ„çš„çˆ¶ç›®éŒ„ï¼ˆå³å°ˆæ¡ˆæ ¹ç›®éŒ„ï¼‰
PROJECT_ROOT = Path(__file__).resolve().parent.parent
TEST_FILE_DIR = PROJECT_ROOT / "test_file"
METRICS_OUTPUT_FILE = str(TEST_FILE_DIR / "control_group_throughput_metrics.csv")

LOG_LEVELS = ["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"]
LOG_MESSAGES = [
    "ç³»çµ±æ­£å¸¸é‹è¡Œ",
    "è¨˜æ†¶é«”ä½¿ç”¨ç‡: {usage}%",
    "CPU æº«åº¦: {temp}Â°C",
    "ç¶²è·¯é€£ç·šç•°å¸¸",
    "è³‡æ–™åº«æŸ¥è©¢è¶…æ™‚",
    "æª”æ¡ˆè®€å–å¤±æ•—",
    "æ„Ÿæ¸¬å™¨è®€æ•¸ç•°å¸¸",
    "æ”å½±æ©Ÿç•«é¢æ¨¡ç³Š",
    "ç¡¬ç¢Ÿç©ºé–“ä¸è¶³",
    "è¨­å‚™é‡æ–°å•Ÿå‹•"
]

# ==========================================
# Prometheus æŒ‡æ¨™æŸ¥è©¢èˆ‡åŒ¯å‡º
# ==========================================
class PrometheusExporter:
    """Prometheus æŒ‡æ¨™æŸ¥è©¢èˆ‡åŒ¯å‡ºå·¥å…·ï¼ˆå°ç…§çµ„ç‰ˆæœ¬ï¼‰"""

    def __init__(self, prometheus_url: str = PROMETHEUS_URL):
        self.prometheus_url = prometheus_url
        self.query_url = urljoin(prometheus_url, "/api/v1/query_range")

    def query_range(self, query: str, start: datetime, end: datetime, step: str = "1s") -> Dict[str, Any]:
        """
        æŸ¥è©¢ Prometheus æ™‚é–“ç¯„åœè³‡æ–™

        Args:
            query: PromQL æŸ¥è©¢èªå¥
            start: é–‹å§‹æ™‚é–“
            end: çµæŸæ™‚é–“
            step: æ™‚é–“é–“éš” (é è¨­ 1s)

        Returns:
            æŸ¥è©¢çµæœ dict
        """
        params = {
            "query": query,
            "start": start.timestamp(),
            "end": end.timestamp(),
            "step": step
        }

        try:
            response = requests.get(self.query_url, params=params, timeout=30)
            response.raise_for_status()
            return response.json()
        except requests.exceptions.RequestException as e:
            print(f"âŒ æŸ¥è©¢å¤±æ•—: {query}")
            print(f"   éŒ¯èª¤: {e}")
            return {"status": "error", "data": {"result": []}}

    def export_throughput_metrics(self, start: datetime, end: datetime, output_file: str = METRICS_OUTPUT_FILE):
        """
        åŒ¯å‡ºå°ç…§çµ„ç³»çµ±ååé‡æŒ‡æ¨™åˆ° CSV

        é€™å€‹æ–¹æ³•æœƒæŸ¥è©¢ control-group-dashboard.json ä¸­ã€Œç³»çµ±ååé‡ (Throughput)ã€
        é¢æ¿å®šç¾©çš„ 3 å€‹æŒ‡æ¨™ (ä½¿ç”¨ rate[30s] å¹³æ»‘å¹³å‡):
        - æ—¥èªŒæ•¸ (logs/s) - 30s å¹³å‡
        - HTTP è«‹æ±‚ (req/s) - 30s å¹³å‡
        - PG æ’å…¥ (rows/s) - 30s å¹³å‡
        """

        # ä¿®æ”¹ï¼šç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
        output_path = Path(output_file)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        # æ“´å±•æ™‚é–“ç¯„åœï¼šé–‹å§‹æ™‚é–“å¾€å‰æ¨ 1 åˆ†é˜ï¼ŒçµæŸæ™‚é–“å¾€å¾Œæ¨ 1 åˆ†é˜
        # é€™æ¨£å¯ä»¥ç¢ºä¿æ“·å–åˆ°å®Œæ•´çš„æ¸¬è©¦è³‡æ–™
        extended_start = start - timedelta(minutes=1)
        extended_end = end + timedelta(minutes=1)

        # å®šç¾©è¦æŸ¥è©¢çš„æŒ‡æ¨™ (ä¾†è‡ª control-group-dashboard.json panel id=0)
        # ä½¿ç”¨ rate[30s] ä»¥ç¬¦åˆå°ç…§çµ„ä½ååé‡ç›£æ§ç‰¹æ€§
        queries = [
            {
                "name": "logs_per_second",
                "query": "sum(rate(logs_received_total[30s]))",
                "description": "æ—¥èªŒæ•¸ (logs/s) - 30s å¹³å‡"
            },
            {
                "name": "http_requests_per_second",
                "query": "sum(rate(http_requests_total[30s]))",
                "description": "HTTP è«‹æ±‚ (req/s) - 30s å¹³å‡"
            },
            {
                "name": "pg_inserts_per_second",
                "query": "sum(rate(pg_stat_database_tup_inserted{datname=\"logsdb\"}[30s]))",
                "description": "PG æ’å…¥ (rows/s) - 30s å¹³å‡"
            }
        ]

        print(f"\nğŸ“Š é–‹å§‹æŸ¥è©¢å°ç…§çµ„ååé‡æŒ‡æ¨™...")
        print(f"   åŸå§‹æ™‚é–“ç¯„åœ: {start} ~ {end}")
        print(f"   æ“´å±•æ™‚é–“ç¯„åœ: {extended_start} ~ {extended_end}")
        print(f"   (å‰å¾Œå„æ“´å±• 1 åˆ†é˜ä»¥ç¢ºä¿è³‡æ–™å®Œæ•´æ€§)")
        print(f"   æŸ¥è©¢æŒ‡æ¨™æ•¸: {len(queries)}")

        # æŸ¥è©¢æ‰€æœ‰æŒ‡æ¨™ï¼ˆä½¿ç”¨æ“´å±•å¾Œçš„æ™‚é–“ç¯„åœï¼‰
        all_data = {}
        timestamps = set()

        for metric in queries:
            print(f"   æŸ¥è©¢: {metric['description']}")
            result = self.query_range(
                metric['query'], extended_start, extended_end, step="1s"
            )

            if result.get("status") == "success" and result.get("data", {}).get("result"):
                # å–å¾—ç¬¬ä¸€å€‹çµæœ (å› ç‚ºä½¿ç”¨ sum() èšåˆ)
                values = result["data"]["result"][0].get("values", [])

                # å°‡è³‡æ–™å­˜å…¥ dictï¼Œä»¥ timestamp ç‚º key
                metric_data = {}
                for ts, value in values:
                    timestamp = datetime.fromtimestamp(ts)
                    timestamps.add(timestamp)
                    metric_data[timestamp] = float(value)

                all_data[metric['name']] = {
                    'description': metric['description'],
                    'data': metric_data
                }
                print(f"      âœ… å–å¾— {len(values)} ç­†è³‡æ–™")
            else:
                print(f"      âš ï¸  ç„¡è³‡æ–™æˆ–æŸ¥è©¢å¤±æ•—")
                all_data[metric['name']] = {
                    'description': metric['description'],
                    'data': {}
                }

        # å¦‚æœæ²’æœ‰ä»»ä½•è³‡æ–™ï¼Œæå‰çµæŸ
        if not timestamps:
            print("âŒ æ²’æœ‰ä»»ä½•è³‡æ–™å¯åŒ¯å‡º")
            print("   è«‹ç¢ºèª:")
            print(f"   1. Prometheus æœå‹™æ˜¯å¦æ­£åœ¨é‹è¡Œ ({self.prometheus_url})")
            print("   2. æ™‚é–“ç¯„åœå…§æ˜¯å¦æœ‰è³‡æ–™")
            print("   3. æŒ‡æ¨™åç¨±æ˜¯å¦æ­£ç¢º")
            return

        # æ’åºæ™‚é–“æˆ³è¨˜
        sorted_timestamps = sorted(timestamps)

        # å¯«å…¥ CSV
        print(f"\nğŸ’¾ å¯«å…¥ CSV: {output_file}")
        with open(output_file, 'w', newline='', encoding='utf-8-sig') as csvfile:
            # æº–å‚™æ¬„ä½åç¨±
            fieldnames = ['timestamp'] + [
                f"{metric['name']} ({metric['description']})"
                for metric in queries
            ]

            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()

            # å¯«å…¥è³‡æ–™
            for ts in sorted_timestamps:
                row = {'timestamp': ts.strftime('%Y-%m-%d %H:%M:%S')}

                for metric in queries:
                    metric_name = metric['name']
                    column_name = f"{metric_name} ({metric['description']})"

                    # å–å¾—è©²æ™‚é–“é»çš„å€¼ï¼Œå¦‚æœæ²’æœ‰å‰‡ç•™ç©º
                    value = all_data[metric_name]['data'].get(ts, '')
                    row[column_name] = value

                writer.writerow(row)

        print(f"âœ… åŒ¯å‡ºå®Œæˆ!")
        print(f"   æª”æ¡ˆ: {output_file}")
        print(f"   è³‡æ–™ç­†æ•¸: {len(sorted_timestamps)}")
        print(f"   æ™‚é–“ç¯„åœ: {sorted_timestamps[0]} ~ {sorted_timestamps[-1]}")
        print()
        print("ğŸ“ˆ çµ±è¨ˆæ‘˜è¦:")
        for metric in queries:
            metric_name = metric['name']
            data_values = list(all_data[metric_name]['data'].values())
            if data_values:
                print(f"   {metric['description']}:")
                print(f"      æœ€å¤§å€¼: {max(data_values):.2f}")
                print(f"      æœ€å°å€¼: {min(data_values):.2f}")
                print(f"      å¹³å‡å€¼: {sum(data_values)/len(data_values):.2f}")

# ==========================================
# ç”Ÿæˆæ¸¬è©¦è³‡æ–™
# ==========================================
def generate_log_data(device_id: str, log_num: int) -> dict:
    """ç”Ÿæˆéš¨æ©Ÿæ—¥èªŒè³‡æ–™"""
    log_level = random.choice(LOG_LEVELS)
    message_template = random.choice(LOG_MESSAGES)

    if "{usage}" in message_template:
        message = message_template.format(usage=random.randint(50, 95))
    elif "{temp}" in message_template:
        message = message_template.format(temp=random.randint(40, 85))
    else:
        message = message_template

    return {
        "device_id": device_id,
        "log_level": log_level,
        "message": f"{message} (#{log_num})",
        "log_data": {
            "test_id": log_num,
            "timestamp": datetime.now().isoformat(),
            "random_value": random.random(),
            "sequence": log_num
        }
    }

# ==========================================
# ç™¼é€å–®ç­†æ—¥èªŒ
# ==========================================
async def send_log(session: aiohttp.ClientSession, device_id: str, log_num: int) -> dict:
    """ç™¼é€å–®ç­†æ—¥èªŒåˆ° API"""
    url = f"{BASE_URL}/api/log"
    log_data = generate_log_data(device_id, log_num)

    start_time = time.time()

    try:
        async with session.post(url, json=log_data, timeout=aiohttp.ClientTimeout(total=30)) as response:
            response_time = (time.time() - start_time) * 1000

            if response.status == 200:
                return {
                    "success": True,
                    "response_time": response_time,
                    "status": response.status,
                    "error": None,
                    "count": 1
                }
            else:
                return {
                    "success": False,
                    "response_time": response_time,
                    "status": response.status,
                    "error": await response.text(),
                    "count": 1
                }

    except asyncio.TimeoutError:
        return {
            "success": False,
            "response_time": (time.time() - start_time) * 1000,
            "status": 0,
            "error": "è«‹æ±‚è¶…æ™‚",
            "count": 1
        }
    except Exception as e:
        return {
            "success": False,
            "response_time": (time.time() - start_time) * 1000,
            "status": 0,
            "error": str(e),
            "count": 1
        }

# ==========================================
# ç™¼é€æ‰¹é‡æ—¥èªŒ
# ==========================================
async def send_batch_logs(session: aiohttp.ClientSession, logs: List[dict]) -> dict:
    """æ‰¹é‡ç™¼é€æ—¥èªŒåˆ° API"""
    url = f"{BASE_URL}/api/logs/batch"
    batch_data = {"logs": logs}

    start_time = time.time()

    try:
        async with session.post(url, json=batch_data, timeout=aiohttp.ClientTimeout(total=60)) as response:
            response_time = (time.time() - start_time) * 1000

            if response.status == 200:
                return {
                    "success": True,
                    "response_time": response_time,
                    "status": response.status,
                    "error": None,
                    "count": len(logs)
                }
            else:
                return {
                    "success": False,
                    "response_time": response_time,
                    "status": response.status,
                    "error": await response.text(),
                    "count": len(logs)
                }

    except asyncio.TimeoutError:
        return {
            "success": False,
            "response_time": (time.time() - start_time) * 1000,
            "status": 0,
            "error": "è«‹æ±‚è¶…æ™‚",
            "count": len(logs)
        }
    except Exception as e:
        return {
            "success": False,
            "response_time": (time.time() - start_time) * 1000,
            "status": 0,
            "error": str(e),
            "count": len(logs)
        }

# ==========================================
# æ‰¹æ¬¡ç™¼é€æ—¥èªŒ
# ==========================================
async def batch_send_logs(
    session: aiohttp.ClientSession,
    device_id: str,
    num_logs: int,
    semaphore: asyncio.Semaphore
) -> List[dict]:
    """æ‰¹æ¬¡ç™¼é€æ—¥èªŒï¼ˆä½¿ç”¨ä¿¡è™Ÿé‡æ§åˆ¶ä¸¦ç™¼ï¼‰"""
    if USE_BATCH_API:
        all_logs = [generate_log_data(device_id, log_num) for log_num in range(num_logs)]
        results = []

        for i in range(0, len(all_logs), BATCH_SIZE):
            batch = all_logs[i:i + BATCH_SIZE]
            async with semaphore:
                result = await send_batch_logs(session, batch)
                results.append(result)

        return results
    else:
        async def send_with_semaphore(log_num: int) -> dict:
            async with semaphore:
                return await send_log(session, device_id, log_num)

        tasks = [send_with_semaphore(log_num) for log_num in range(num_logs)]
        return await asyncio.gather(*tasks)

# ==========================================
# ä¸»è¦å£“åŠ›æ¸¬è©¦
# ==========================================
async def stress_test(
    num_devices: int = NUM_DEVICES,
    logs_per_device: int = LOGS_PER_DEVICE,
    concurrent_limit: int = CONCURRENT_LIMIT,
    iteration: int = 1,
    current_iteration: int = 1
):
    """åŸ·è¡Œå£“åŠ›æ¸¬è©¦"""
    print("=" * 70)
    if iteration > 1:
        print(f"  ğŸ“Š å°ç…§çµ„ - ç°¡åŒ–ç³»çµ±å£“åŠ›æ¸¬è©¦ [ç¬¬ {current_iteration}/{iteration} è¼ª]")
    else:
        print("  ğŸ“Š å°ç…§çµ„ - ç°¡åŒ–ç³»çµ±å£“åŠ›æ¸¬è©¦")
    print("=" * 70)
    print(f"æ¸¬è©¦é…ç½®ï¼š")
    print(f"  â€¢ è¨­å‚™æ•¸é‡: {num_devices}")
    print(f"  â€¢ æ¯å°è¨­å‚™æ—¥èªŒæ•¸: {logs_per_device}")
    print(f"  â€¢ ç¸½æ—¥èªŒæ•¸: {num_devices * logs_per_device:,}")
    print(f"  â€¢ ä¸¦ç™¼é™åˆ¶: {concurrent_limit}")
    print(f"  â€¢ API ç«¯é»: {BASE_URL}")
    print(f"  â€¢ ç³»çµ±ç‰¹æ€§: ç„¡ Nginxã€é€£æ¥æ± ã€Redisã€Worker")
    if iteration > 1:
        print(f"  â€¢ ç¸½å¾ªç’°æ¬¡æ•¸: {iteration}")
        print(f"  â€¢ ç•¶å‰å¾ªç’°: {current_iteration}")
    print("-" * 70)

    semaphore = asyncio.Semaphore(concurrent_limit)

    # è¨˜éŒ„æ¸¬è©¦é–‹å§‹æ™‚é–“ï¼ˆç”¨æ–¼ Prometheus æŸ¥è©¢ï¼‰
    test_start_datetime = datetime.now()
    start_time = time.time()

    connector = aiohttp.TCPConnector(limit=concurrent_limit, limit_per_host=concurrent_limit)
    timeout = aiohttp.ClientTimeout(total=600)  # 10åˆ†é˜è¶…æ™‚ï¼ˆç°¡åŒ–ç‰ˆè¼ƒæ…¢ï¼‰

    async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
        device_tasks = []

        for device_num in range(num_devices):
            # ä¿®æ”¹ï¼šåŠ å…¥ 'control_' å‰ç¶´ä»¥å€åˆ†å°ç…§çµ„æ¸¬è©¦è³‡æ–™
            device_id = f"control_device_{device_num:03d}"
            task = batch_send_logs(session, device_id, logs_per_device, semaphore)
            device_tasks.append(task)

        print("â³ é–‹å§‹ç™¼é€æ—¥èªŒ...")
        all_results = await asyncio.gather(*device_tasks)

    total_time = time.time() - start_time
    # è¨˜éŒ„æ¸¬è©¦çµæŸæ™‚é–“ï¼ˆç”¨æ–¼ Prometheus æŸ¥è©¢ï¼‰
    test_end_datetime = datetime.now()

    # æ•´ç†çµæœ
    all_responses = [result for device_results in all_results for result in device_results]

    total_requests = len(all_responses)
    successful_requests = sum(1 for r in all_responses if r["success"])
    failed_requests = total_requests - successful_requests
    total_logs_sent = sum(r.get("count", 1) for r in all_responses)
    successful_logs = sum(r.get("count", 1) for r in all_responses if r["success"])

    response_times = [r["response_time"] for r in all_responses if r["success"]]

    if response_times:
        avg_response_time = sum(response_times) / len(response_times)
        min_response_time = min(response_times)
        max_response_time = max(response_times)

        sorted_times = sorted(response_times)
        p50 = sorted_times[int(len(sorted_times) * 0.50)]
        p95 = sorted_times[int(len(sorted_times) * 0.95)]
        p99 = sorted_times[int(len(sorted_times) * 0.99)]
    else:
        avg_response_time = 0
        min_response_time = 0
        max_response_time = 0
        p50 = p95 = p99 = 0

    throughput = successful_logs / total_time if total_time > 0 else 0

    # è¼¸å‡ºçµæœ
    print("\n" + "=" * 70)
    print("  ğŸ“ˆ æ¸¬è©¦çµæœ")
    print("=" * 70)

    print(f"\nâ±ï¸  æ™‚é–“çµ±è¨ˆï¼š")
    print(f"  â€¢ ç¸½è€—æ™‚: {total_time:.2f} ç§’")

    print(f"\nğŸ“Š è«‹æ±‚çµ±è¨ˆï¼š")
    if USE_BATCH_API:
        print(f"  â€¢ æ‰¹é‡è«‹æ±‚æ•¸: {total_requests:,}")
        print(f"  â€¢ ç¸½æ—¥èªŒæ•¸: {total_logs_sent:,}")
        print(f"  â€¢ æˆåŠŸæ—¥èªŒ: {successful_logs:,} ({successful_logs/total_logs_sent*100:.1f}%)")
    else:
        print(f"  â€¢ ç¸½è«‹æ±‚æ•¸: {total_requests:,}")
    print(f"  â€¢ æˆåŠŸè«‹æ±‚: {successful_requests:,} ({successful_requests/total_requests*100:.1f}%)")
    print(f"  â€¢ å¤±æ•—è«‹æ±‚: {failed_requests:,} ({failed_requests/total_requests*100:.1f}%)")

    print(f"\nâš¡ æ•ˆèƒ½æŒ‡æ¨™ï¼š")
    print(f"  â€¢ ååé‡: {throughput:.2f} logs/ç§’")
    print(f"  â€¢ å¹³å‡å›æ‡‰æ™‚é–“: {avg_response_time:.2f} ms")
    print(f"  â€¢ æœ€å°å›æ‡‰æ™‚é–“: {min_response_time:.2f} ms")
    print(f"  â€¢ æœ€å¤§å›æ‡‰æ™‚é–“: {max_response_time:.2f} ms")

    print(f"\nğŸ“‰ ç™¾åˆ†ä½æ•¸ï¼š")
    print(f"  â€¢ P50 (ä¸­ä½æ•¸): {p50:.2f} ms")
    print(f"  â€¢ P95: {p95:.2f} ms")
    print(f"  â€¢ P99: {p99:.2f} ms")

    if failed_requests > 0:
        print(f"\nâŒ éŒ¯èª¤åˆ†æï¼š")
        error_types = {}
        for r in all_responses:
            if not r["success"]:
                error = r["error"] or f"HTTP {r['status']}"
                error_types[error] = error_types.get(error, 0) + 1

        for error, count in sorted(error_types.items(), key=lambda x: x[1], reverse=True):
            print(f"  â€¢ {error}: {count} æ¬¡")

    print("\n" + "=" * 70)

    target_throughput = 10000
    target_p95 = 100

    print(f"\nğŸ¯ ç›®æ¨™é”æˆæƒ…æ³ï¼š")

    if throughput >= target_throughput:
        print(f"  âœ… ååé‡é”æ¨™: {throughput:.2f} >= {target_throughput} logs/ç§’")
    else:
        print(f"  âŒ ååé‡æœªé”æ¨™: {throughput:.2f} < {target_throughput} logs/ç§’")

    if p95 <= target_p95:
        print(f"  âœ… P95 å›æ‡‰æ™‚é–“é”æ¨™: {p95:.2f} <= {target_p95} ms")
    else:
        print(f"  âŒ P95 å›æ‡‰æ™‚é–“æœªé”æ¨™: {p95:.2f} > {target_p95} ms")

    if failed_requests == 0:
        print(f"  âœ… ç„¡å¤±æ•—è«‹æ±‚")
    else:
        print(f"  âš ï¸ æœ‰ {failed_requests} å€‹å¤±æ•—è«‹æ±‚")

    print("=" * 70)

    # ==========================================
    # åŒ¯å‡º Prometheus æŒ‡æ¨™
    # ==========================================
    # ä¿®æ”¹ï¼šåªåœ¨æœ€å¾Œä¸€è¼ªæ¸¬è©¦å®Œæˆæ™‚æ‰åŒ¯å‡ºæŒ‡æ¨™ï¼ˆæ•´åˆæ‰€æœ‰æ¸¬è©¦æ•¸æ“šï¼‰
    # if EXPORT_METRICS:
    #     try:
    #         print("\n" + "=" * 70)
    #         print("  ğŸ“Š åŒ¯å‡º Prometheus ååé‡æŒ‡æ¨™")
    #         print("=" * 70)
    #
    #         exporter = PrometheusExporter(PROMETHEUS_URL)
    #
    #         # ç‚ºæ¯å€‹å¾ªç’°ç”Ÿæˆå”¯ä¸€çš„æª”æ¡ˆåç¨±
    #         if iteration > 1:
    #             output_file = f"control_group_throughput_metrics_iter{current_iteration:02d}.csv"
    #         else:
    #             output_file = METRICS_OUTPUT_FILE
    #
    #         exporter.export_throughput_metrics(
    #             test_start_datetime,
    #             test_end_datetime,
    #             output_file
    #         )
    #
    #         print("=" * 70)
    #     except Exception as e:
    #         print(f"\nâš ï¸  æŒ‡æ¨™åŒ¯å‡ºå¤±æ•—: {e}")
    #         print("   æ¸¬è©¦çµæœä¸å—å½±éŸ¿ï¼Œå¯æ‰‹å‹•åŒ¯å‡ºæŒ‡æ¨™")

    # æ”¹ç‚ºè¿”å›æ¸¬è©¦æ™‚é–“ç¯„åœï¼Œä¾›ä¸»ç¨‹å¼åŒ¯å‡ºä½¿ç”¨
    return test_start_datetime, test_end_datetime

# ==========================================
# ä¸»ç¨‹å¼
# ==========================================
async def main():
    """ä¸»ç¨‹å¼å…¥å£"""
    # ä¿®æ”¹ï¼šè¨˜éŒ„æ‰€æœ‰æ¸¬è©¦çš„æ™‚é–“ç¯„åœ
    all_test_start = None
    all_test_end = None

    for i in range(NUM_ITERATIONS):
        test_start, test_end = await stress_test(
            num_devices=NUM_DEVICES,
            logs_per_device=LOGS_PER_DEVICE,
            concurrent_limit=CONCURRENT_LIMIT,
            iteration=NUM_ITERATIONS,
            current_iteration=i + 1
        )

        # è¨˜éŒ„ç¬¬ä¸€æ¬¡æ¸¬è©¦çš„é–‹å§‹æ™‚é–“
        if all_test_start is None:
            all_test_start = test_start

        # æ›´æ–°æœ€å¾Œä¸€æ¬¡æ¸¬è©¦çš„çµæŸæ™‚é–“
        all_test_end = test_end

        if i < NUM_ITERATIONS - 1 and ITERATION_INTERVAL > 0:
            print(f"\nâ¸ï¸  ç­‰å¾… {ITERATION_INTERVAL} ç§’å¾Œé–‹å§‹ä¸‹ä¸€è¼ªæ¸¬è©¦...")
            await asyncio.sleep(ITERATION_INTERVAL)

    # ä¿®æ”¹ï¼šæ‰€æœ‰æ¸¬è©¦å®Œæˆå¾Œï¼ŒåŒ¯å‡ºæ•´åˆçš„ Prometheus æŒ‡æ¨™åˆ°å–®ä¸€ CSV æª”æ¡ˆ
    if EXPORT_METRICS and all_test_start and all_test_end:
        try:
            print("\n" + "=" * 70)
            print("  ğŸ“Š åŒ¯å‡ºæ‰€æœ‰æ¸¬è©¦çš„ Prometheus ååé‡æŒ‡æ¨™ï¼ˆæ•´åˆç‰ˆï¼‰")
            print("=" * 70)
            print(f"  â€¢ æ¸¬è©¦è¼ªæ•¸: {NUM_ITERATIONS}")
            print(f"  â€¢ ç¸½æ™‚é–“ç¯„åœ: {all_test_start} ~ {all_test_end}")
            print(f"  â€¢ è¼¸å‡ºæª”æ¡ˆ: {METRICS_OUTPUT_FILE}")
            print("=" * 70)

            exporter = PrometheusExporter(PROMETHEUS_URL)
            exporter.export_throughput_metrics(
                all_test_start,
                all_test_end,
                METRICS_OUTPUT_FILE
            )

            print("=" * 70)
        except Exception as e:
            print(f"\nâš ï¸  æŒ‡æ¨™åŒ¯å‡ºå¤±æ•—: {e}")
            print("   æ¸¬è©¦çµæœä¸å—å½±éŸ¿ï¼Œå¯æ‰‹å‹•åŒ¯å‡ºæŒ‡æ¨™")

    print("\nâœ… æ¸¬è©¦å®Œæˆ")

if __name__ == "__main__":
    asyncio.run(main())
