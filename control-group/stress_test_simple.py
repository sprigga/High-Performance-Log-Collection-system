"""
å°ç…§çµ„å£“åŠ›æ¸¬è©¦è…³æœ¬ - æ¸¬è©¦ç°¡åŒ–ç‰ˆç³»çµ±
ç›´æ¥å¯«å…¥ PostgreSQLï¼Œç„¡è² è¼‰å¹³è¡¡ã€é€£æ¥æ± ã€Redisã€Worker

æ•´åˆåŠŸèƒ½ï¼š
- å£“åŠ›æ¸¬è©¦åŸ·è¡Œ
- Prometheus æŒ‡æ¨™è‡ªå‹•æ“·å–èˆ‡åŒ¯å‡º
"""
import asyncio
import aiohttp
import time
import random
import csv
import requests
import os
import json  # æ–°å¢ï¼šJSON åŒ¯å‡ºåŠŸèƒ½
from datetime import datetime, timedelta
from typing import List, Dict, Any
from urllib.parse import urljoin
from pathlib import Path

# ==========================================
# æ¸¬è©¦é…ç½®
# ==========================================
BASE_URL = "http://localhost:18724"  # å°ç…§çµ„ç«¯é»
NUM_DEVICES = 100                    # è¨­å‚™æ•¸é‡
LOGS_PER_DEVICE = 100                # æ¯å°è¨­å‚™ç™¼é€çš„æ—¥èªŒæ•¸
CONCURRENT_LIMIT = 200               # ä¸¦ç™¼é™åˆ¶
BATCH_SIZE = 5                       # æ‰¹æ¬¡å¤§å°
USE_BATCH_API = True                 # æ˜¯å¦ä½¿ç”¨æ‰¹é‡ API
NUM_ITERATIONS = 50                 # æ¸¬è©¦åŸ·è¡Œçš„å¾ªç’°æ¬¡æ•¸
ITERATION_INTERVAL = 5               # æ¯æ¬¡å¾ªç’°ä¹‹é–“çš„é–“éš”æ™‚é–“ï¼ˆç§’ï¼‰

# Prometheus ç›£æ§é…ç½®
PROMETHEUS_URL = "http://localhost:19090"  # å°ç…§çµ„ Prometheus ç«¯é»
EXPORT_METRICS = True                # æ˜¯å¦è‡ªå‹•åŒ¯å‡ºæŒ‡æ¨™

# æ–°å¢ï¼šPrometheus API å®¢æˆ¶ç«¯å¯ç”¨æ€§æª¢æŸ¥
try:
    from prometheus_api_client import PrometheusConnect
    PROMETHEUS_AVAILABLE = True
except ImportError:
    PROMETHEUS_AVAILABLE = False
    print("âš ï¸  è­¦å‘Š: prometheus_api_client æœªå®‰è£ï¼ŒPrometheus æŒ‡æ¨™æŸ¥è©¢åŠŸèƒ½å°‡è¢«åœç”¨")

# ä¿®æ”¹ï¼šä½¿ç”¨ç›¸å°è·¯å¾‘ï¼Œå‹•æ…‹è¨ˆç®—å°ˆæ¡ˆæ ¹ç›®éŒ„ä¸‹çš„ test_file/ ç›®éŒ„
# å–å¾—è…³æœ¬æ‰€åœ¨ç›®éŒ„çš„çˆ¶ç›®éŒ„ï¼ˆå³å°ˆæ¡ˆæ ¹ç›®éŒ„ï¼‰
PROJECT_ROOT = Path(__file__).resolve().parent.parent
TEST_FILE_DIR = PROJECT_ROOT / "test_file"
METRICS_OUTPUT_FILE = str(TEST_FILE_DIR / "control_group_throughput_metrics.csv")

LOG_LEVELS = ["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"]
LOG_MESSAGES = [
    "ç³»çµ±æ­£å¸¸é‹è¡Œ",
    "è¨˜æ†¶é«”ä½¿ç”¨ç‡: {usage}%",
    "CPU æº«åº¦: {temp}Â°C",
    "ç¶²è·¯é€£ç·šç•°å¸¸",
    "è³‡æ–™åº«æŸ¥è©¢è¶…æ™‚",
    "æª”æ¡ˆè®€å–å¤±æ•—",
    "æ„Ÿæ¸¬å™¨è®€æ•¸ç•°å¸¸",
    "æ”å½±æ©Ÿç•«é¢æ¨¡ç³Š",
    "ç¡¬ç¢Ÿç©ºé–“ä¸è¶³",
    "è¨­å‚™é‡æ–°å•Ÿå‹•"
]

# ==========================================
# æ–°å¢ï¼šPrometheus æŒ‡æ¨™æŸ¥è©¢å™¨é¡åˆ¥ï¼ˆæ•´åˆè‡ª query_prometheus_metrics.pyï¼‰
# ==========================================
class PrometheusMetricsQuerier:
    """
    Prometheus æŒ‡æ¨™æŸ¥è©¢å™¨ï¼ˆæ•´åˆè‡ª query_prometheus_metrics.pyï¼‰
    """
    def __init__(self, prometheus_url=PROMETHEUS_URL):
        """
        åˆå§‹åŒ– Prometheus é€£æ¥

        Args:
            prometheus_url (str): Prometheus æœå‹™çš„ URL
        """
        if not PROMETHEUS_AVAILABLE:
            self.prometheus = None
            return

        try:
            self.prometheus = PrometheusConnect(url=prometheus_url, disable_ssl=True)
            self.prometheus_url = prometheus_url
        except Exception as e:
            print(f"âš ï¸  ç„¡æ³•é€£æ¥åˆ° Prometheus: {e}")
            self.prometheus = None

    def test_connection(self):
        """æ¸¬è©¦èˆ‡ Prometheus çš„é€£æ¥"""
        if not self.prometheus:
            return False

        try:
            # å˜—è©¦ç²å–ä¸€å€‹åŸºæœ¬æŒ‡æ¨™ä¾†æ¸¬è©¦é€£æ¥
            result = self.prometheus.get_current_metric_value("up")
            return True
        except Exception as e:
            print(f"âš ï¸  ç„¡æ³•é€£æ¥åˆ° Prometheus: {e}")
            return False

    def query_test_metrics(self, start_time, end_time, batch_size=BATCH_SIZE):
        """
        æŸ¥è©¢æ¸¬è©¦æœŸé–“çš„ Prometheus æŒ‡æ¨™

        Args:
            start_time (datetime): æ¸¬è©¦é–‹å§‹æ™‚é–“
            end_time (datetime): æ¸¬è©¦çµæŸæ™‚é–“
            batch_size (int): æ‰¹æ¬¡å¤§å°

        Returns:
            dict: åŒ…å«æŸ¥è©¢çµæœçš„å­—å…¸
        """
        if not self.prometheus:
            return {"error": "Prometheus ä¸å¯ç”¨"}

        metrics = {}

        try:
            # ç²å–ç«¯é»æ¨™ç±¤åç¨±
            all_requests_result = self.prometheus.get_current_metric_value("http_requests_total")
            endpoint_label = 'endpoint'
            if all_requests_result:
                sample_labels = all_requests_result[0].get('metric', {})
                endpoint_label = 'endpoint' if 'endpoint' in sample_labels else 'handler' if 'handler' in sample_labels else 'endpoint'
        except:
            endpoint_label = 'endpoint'

        # æŸ¥è©¢æ™‚é–“ç¯„åœå…§çš„æŒ‡æ¨™
        try:
            # QPS (æ‰€æœ‰ç«¯é»)
            qps_result = self.prometheus.custom_query_range(
                query='rate(http_requests_total[1m])',
                start_time=start_time,
                end_time=end_time,
                step='1s'
            )

            # è¨ˆç®—æœ€å¤§å’Œå¹³å‡ QPS
            max_qps_values = []
            for result in qps_result:
                if 'values' in result:
                    values = [float(value[1]) for value in result['values'] if value[1] not in ['NaN', None]]
                    if values:
                        max_qps_values.extend(values)

            metrics['qps'] = {
                'max': max(max_qps_values, default=0) if max_qps_values else 0,
                'avg': sum(max_qps_values) / len(max_qps_values) if max_qps_values else 0
            }
        except Exception as e:
            print(f"âš ï¸  æŸ¥è©¢ QPS æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            metrics['qps'] = {'max': 0, 'avg': 0}

        try:
            # æ‰¹é‡ç«¯é» QPS
            qps_batch_query = f'rate(http_requests_total{{{endpoint_label}="/api/logs/batch"}}[1m])'
            qps_batch_result = self.prometheus.custom_query_range(
                query=qps_batch_query,
                start_time=start_time,
                end_time=end_time,
                step='1s'
            )

            max_qps_batch_values = []
            for result in qps_batch_result:
                if 'values' in result:
                    values = [float(value[1]) for value in result['values'] if value[1] not in ['NaN', None]]
                    if values:
                        max_qps_batch_values.extend(values)

            metrics['qps_batch'] = {
                'max': max(max_qps_batch_values, default=0) if max_qps_batch_values else 0,
                'avg': sum(max_qps_batch_values) / len(max_qps_batch_values) if max_qps_batch_values else 0
            }
        except Exception as e:
            print(f"âš ï¸  æŸ¥è©¢æ‰¹é‡ç«¯é» QPS æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            metrics['qps_batch'] = {'max': 0, 'avg': 0}

        try:
            # ååé‡ (Logs/s)
            throughput_query = f'rate(http_requests_total{{{endpoint_label}="/api/logs/batch"}}[1m]) * {batch_size}'
            throughput_result = self.prometheus.custom_query_range(
                query=throughput_query,
                start_time=start_time,
                end_time=end_time,
                step='1s'
            )

            max_throughput_values = []
            for result in throughput_result:
                if 'values' in result:
                    values = [float(value[1]) for value in result['values'] if value[1] not in ['NaN', None]]
                    if values:
                        max_throughput_values.extend(values)

            metrics['throughput'] = {
                'max': max(max_throughput_values, default=0) if max_throughput_values else 0,
                'avg': sum(max_throughput_values) / len(max_throughput_values) if max_throughput_values else 0
            }
        except Exception as e:
            print(f"âš ï¸  æŸ¥è©¢ååé‡æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            metrics['throughput'] = {'max': 0, 'avg': 0}

        try:
            # P95 éŸ¿æ‡‰æ™‚é–“
            p95_query = 'histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))'
            p95_result = self.prometheus.custom_query_range(
                query=p95_query,
                start_time=start_time,
                end_time=end_time,
                step='1s'
            )

            p95_values = []
            for result in p95_result:
                if 'values' in result:
                    values = [float(value[1]) * 1000 for value in result['values'] if value[1] not in ['NaN', None]]  # è½‰æ›ç‚º ms
                    if values:
                        p95_values.extend(values)

            metrics['p95_response_time'] = {
                'max': max(p95_values, default=0) if p95_values else 0,
                'avg': sum(p95_values) / len(p95_values) if p95_values else 0
            }
        except Exception as e:
            print(f"âš ï¸  æŸ¥è©¢ P95 éŸ¿æ‡‰æ™‚é–“æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            metrics['p95_response_time'] = {'max': 0, 'avg': 0}

        try:
            # P99 éŸ¿æ‡‰æ™‚é–“
            p99_query = 'histogram_quantile(0.99, rate(http_request_duration_seconds_bucket[5m]))'
            p99_result = self.prometheus.custom_query_range(
                query=p99_query,
                start_time=start_time,
                end_time=end_time,
                step='1s'
            )

            p99_values = []
            for result in p99_result:
                if 'values' in result:
                    values = [float(value[1]) * 1000 for value in result['values'] if value[1] not in ['NaN', None]]  # è½‰æ›ç‚º ms
                    if values:
                        p99_values.extend(values)

            metrics['p99_response_time'] = {
                'max': max(p99_values, default=0) if p99_values else 0,
                'avg': sum(p99_values) / len(p99_values) if p99_values else 0
            }
        except Exception as e:
            print(f"âš ï¸  æŸ¥è©¢ P99 éŸ¿æ‡‰æ™‚é–“æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            metrics['p99_response_time'] = {'max': 0, 'avg': 0}

        try:
            # éŒ¯èª¤ç‡
            error_rate_query = 'rate(http_requests_total{status=~"5..|4.."}[1m])'
            error_rate_result = self.prometheus.custom_query_range(
                query=error_rate_query,
                start_time=start_time,
                end_time=end_time,
                step='1s'
            )

            error_rate_values = []
            for result in error_rate_result:
                if 'values' in result:
                    values = [float(value[1]) for value in result['values'] if value[1] not in ['NaN', None]]
                    if values:
                        error_rate_values.extend(values)

            metrics['error_rate'] = {
                'max': max(error_rate_values, default=0) if error_rate_values else 0,
                'avg': sum(error_rate_values) / len(error_rate_values) if error_rate_values else 0
            }
        except Exception as e:
            print(f"âš ï¸  æŸ¥è©¢éŒ¯èª¤ç‡æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            metrics['error_rate'] = {'max': 0, 'avg': 0}

        return metrics

# ==========================================
# Prometheus æŒ‡æ¨™æŸ¥è©¢èˆ‡åŒ¯å‡º
# ==========================================
class PrometheusExporter:
    """Prometheus æŒ‡æ¨™æŸ¥è©¢èˆ‡åŒ¯å‡ºå·¥å…·ï¼ˆå°ç…§çµ„ç‰ˆæœ¬ï¼‰"""

    def __init__(self, prometheus_url: str = PROMETHEUS_URL):
        self.prometheus_url = prometheus_url
        self.query_url = urljoin(prometheus_url, "/api/v1/query_range")

    def query_range(self, query: str, start: datetime, end: datetime, step: str = "1s") -> Dict[str, Any]:
        """
        æŸ¥è©¢ Prometheus æ™‚é–“ç¯„åœè³‡æ–™

        Args:
            query: PromQL æŸ¥è©¢èªå¥
            start: é–‹å§‹æ™‚é–“
            end: çµæŸæ™‚é–“
            step: æ™‚é–“é–“éš” (é è¨­ 1s)

        Returns:
            æŸ¥è©¢çµæœ dict
        """
        params = {
            "query": query,
            "start": start.timestamp(),
            "end": end.timestamp(),
            "step": step
        }

        try:
            response = requests.get(self.query_url, params=params, timeout=30)
            response.raise_for_status()
            return response.json()
        except requests.exceptions.RequestException as e:
            print(f"âŒ æŸ¥è©¢å¤±æ•—: {query}")
            print(f"   éŒ¯èª¤: {e}")
            return {"status": "error", "data": {"result": []}}

    def export_throughput_metrics(self, start: datetime, end: datetime, output_file: str = METRICS_OUTPUT_FILE):
        """
        åŒ¯å‡ºå°ç…§çµ„ç³»çµ±ååé‡æŒ‡æ¨™åˆ° CSV

        é€™å€‹æ–¹æ³•æœƒæŸ¥è©¢ control-group-dashboard.json ä¸­ã€Œç³»çµ±ååé‡ (Throughput)ã€
        é¢æ¿å®šç¾©çš„ 3 å€‹æŒ‡æ¨™ (ä½¿ç”¨ rate[30s] å¹³æ»‘å¹³å‡):
        - æ—¥èªŒæ•¸ (logs/s) - 30s å¹³å‡
        - HTTP è«‹æ±‚ (req/s) - 30s å¹³å‡
        - PG æ’å…¥ (rows/s) - 30s å¹³å‡
        """

        # ä¿®æ”¹ï¼šç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
        output_path = Path(output_file)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        # æ“´å±•æ™‚é–“ç¯„åœï¼šé–‹å§‹æ™‚é–“å¾€å‰æ¨ 1 åˆ†é˜ï¼ŒçµæŸæ™‚é–“å¾€å¾Œæ¨ 1 åˆ†é˜
        # é€™æ¨£å¯ä»¥ç¢ºä¿æ“·å–åˆ°å®Œæ•´çš„æ¸¬è©¦è³‡æ–™
        extended_start = start - timedelta(minutes=1)
        extended_end = end + timedelta(minutes=1)

        # å®šç¾©è¦æŸ¥è©¢çš„æŒ‡æ¨™ (æ ¸å¿ƒæ•ˆèƒ½æ¯”è¼ƒæŒ‡æ¨™)
        # ä¿®æ”¹èªªæ˜ï¼šä½¿ç”¨ rate[30s] ä»¥ç¢ºä¿èƒ½æŸ¥è©¢åˆ°æ­·å²æ•¸æ“š
        # irate[5s] åƒ…é©ç”¨æ–¼å³æ™‚ç›£æ§ï¼Œç„¡æ³•æŸ¥è©¢æ­·å²æ™‚é–“ç¯„åœçš„æ•¸æ“š
        # ä¸‰å¤§æ ¸å¿ƒæŒ‡æ¨™ï¼š1ï¸âƒ£æ—¥èªŒååé‡ã€2ï¸âƒ£HTTP QPSã€3ï¸âƒ£PGæ’å…¥é€Ÿç‡
        queries = [
            {
                "name": "1_logs_throughput",
                "query": "sum(rate(logs_received_total[30s]))",
                "description": "1ï¸âƒ£ æ—¥èªŒååé‡ (logs/s)"
            },
            {
                "name": "2_http_qps",
                "query": "sum(rate(http_requests_total[30s]))",
                "description": "2ï¸âƒ£ HTTP QPS (req/s)"
            },
            {
                "name": "3_pg_insert_rate",
                "query": "sum(rate(pg_stat_database_tup_inserted{datname=\"logsdb\"}[30s]))",
                "description": "3ï¸âƒ£ PG æ’å…¥é€Ÿç‡ (rows/s)"
            }
        ]

        print(f"\nğŸ“Š é–‹å§‹æŸ¥è©¢å°ç…§çµ„ååé‡æŒ‡æ¨™...")
        print(f"   åŸå§‹æ™‚é–“ç¯„åœ: {start} ~ {end}")
        print(f"   æ“´å±•æ™‚é–“ç¯„åœ: {extended_start} ~ {extended_end}")
        print(f"   (å‰å¾Œå„æ“´å±• 1 åˆ†é˜ä»¥ç¢ºä¿è³‡æ–™å®Œæ•´æ€§)")
        print(f"   æŸ¥è©¢æŒ‡æ¨™æ•¸: {len(queries)}")

        # æŸ¥è©¢æ‰€æœ‰æŒ‡æ¨™ï¼ˆä½¿ç”¨æ“´å±•å¾Œçš„æ™‚é–“ç¯„åœï¼‰
        all_data = {}
        timestamps = set()

        for metric in queries:
            print(f"   æŸ¥è©¢: {metric['description']}")
            result = self.query_range(
                metric['query'], extended_start, extended_end, step="1s"
            )

            if result.get("status") == "success" and result.get("data", {}).get("result"):
                # å–å¾—ç¬¬ä¸€å€‹çµæœ (å› ç‚ºä½¿ç”¨ sum() èšåˆ)
                values = result["data"]["result"][0].get("values", [])

                # å°‡è³‡æ–™å­˜å…¥ dictï¼Œä»¥ timestamp ç‚º key
                metric_data = {}
                for ts, value in values:
                    timestamp = datetime.fromtimestamp(ts)
                    timestamps.add(timestamp)
                    metric_data[timestamp] = float(value)

                all_data[metric['name']] = {
                    'description': metric['description'],
                    'data': metric_data
                }
                print(f"      âœ… å–å¾— {len(values)} ç­†è³‡æ–™")
            else:
                print(f"      âš ï¸  ç„¡è³‡æ–™æˆ–æŸ¥è©¢å¤±æ•—")
                all_data[metric['name']] = {
                    'description': metric['description'],
                    'data': {}
                }

        # å¦‚æœæ²’æœ‰ä»»ä½•è³‡æ–™ï¼Œæå‰çµæŸ
        if not timestamps:
            print("âŒ æ²’æœ‰ä»»ä½•è³‡æ–™å¯åŒ¯å‡º")
            print("   è«‹ç¢ºèª:")
            print(f"   1. Prometheus æœå‹™æ˜¯å¦æ­£åœ¨é‹è¡Œ ({self.prometheus_url})")
            print("   2. æ™‚é–“ç¯„åœå…§æ˜¯å¦æœ‰è³‡æ–™")
            print("   3. æŒ‡æ¨™åç¨±æ˜¯å¦æ­£ç¢º")
            return

        # æ’åºæ™‚é–“æˆ³è¨˜
        sorted_timestamps = sorted(timestamps)

        # å¯«å…¥ CSV
        print(f"\nğŸ’¾ å¯«å…¥ CSV: {output_file}")
        with open(output_file, 'w', newline='', encoding='utf-8-sig') as csvfile:
            # æº–å‚™æ¬„ä½åç¨±
            fieldnames = ['timestamp'] + [
                f"{metric['name']} ({metric['description']})"
                for metric in queries
            ]

            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()

            # å¯«å…¥è³‡æ–™
            for ts in sorted_timestamps:
                row = {'timestamp': ts.strftime('%Y-%m-%d %H:%M:%S')}

                for metric in queries:
                    metric_name = metric['name']
                    column_name = f"{metric_name} ({metric['description']})"

                    # å–å¾—è©²æ™‚é–“é»çš„å€¼ï¼Œå¦‚æœæ²’æœ‰å‰‡ç•™ç©º
                    value = all_data[metric_name]['data'].get(ts, '')
                    row[column_name] = value

                writer.writerow(row)

        print(f"âœ… åŒ¯å‡ºå®Œæˆ!")
        print(f"   æª”æ¡ˆ: {output_file}")
        print(f"   è³‡æ–™ç­†æ•¸: {len(sorted_timestamps)}")
        print(f"   æ™‚é–“ç¯„åœ: {sorted_timestamps[0]} ~ {sorted_timestamps[-1]}")
        print()
        print("ğŸ“ˆ çµ±è¨ˆæ‘˜è¦:")
        for metric in queries:
            metric_name = metric['name']
            data_values = list(all_data[metric_name]['data'].values())
            if data_values:
                print(f"   {metric['description']}:")
                print(f"      æœ€å¤§å€¼: {max(data_values):.2f}")
                print(f"      æœ€å°å€¼: {min(data_values):.2f}")
                print(f"      å¹³å‡å€¼: {sum(data_values)/len(data_values):.2f}")
        # ä¿®æ”¹ï¼šä½¿ç”¨ HTTP QPS (2_http_qps) æ¬„ä½ç¯©é¸ä¸¦å–å‰ 20 ç­†
        # åŸç¨‹å¼ç¢¼ï¼ˆå·²è¨»é‡‹ï¼‰ï¼šä½¿ç”¨ä¸­ä½æ•¸ç¯©é¸
        import pandas as pd

        http_qps_data = all_data.get('2_http_qps', {}).get('data', {})
        if http_qps_data:
            print()
            print("ğŸ” é–‹å§‹é€²è¡Œ HTTP QPS Top 20 ç¯©é¸...")

            # æ’é™¤é›¶å€¼ã€ç©ºå€¼å’Œnullå€¼ï¼Œä¸¦æŒ‰ HTTP QPS é™åºæ’åº
            # å»ºç«‹ (timestamp, http_qps_value) çš„åˆ—è¡¨
            valid_data = [
                (ts, v) for ts, v in http_qps_data.items()
                if v is not None and v != '' and v > 0
            ]

            if not valid_data:
                print("   âš ï¸  æ‰€æœ‰ 2_http_qps è³‡æ–™éƒ½æ˜¯é›¶å€¼/ç©ºå€¼/nullï¼Œç„¡æ³•é€²è¡Œç¯©é¸")
                print()
                print("âš ï¸  ç„¡æ³•é€²è¡Œç¯©é¸: æ²’æœ‰æœ‰æ•ˆçš„ 2_http_qps è³‡æ–™")
                return

            # ä½¿ç”¨ pandas æ’åºä¸¦å–å‰ 20 ç­†
            df_temp = pd.DataFrame(valid_data, columns=['timestamp', 'http_qps'])
            df_sorted = df_temp.sort_values(by='http_qps', ascending=False)
            df_top20 = df_sorted.head(20)

            print(f"   åŸå§‹è³‡æ–™ç­†æ•¸: {len(http_qps_data)}")
            print(f"   éé›¶è³‡æ–™ç­†æ•¸: {len(valid_data)}")
            print(f"   ç¯©é¸å¾Œç­†æ•¸: {len(df_top20)}")
            print(f"   HTTP QPS ç¯„åœ: {df_top20['http_qps'].min():.2f} ~ {df_top20['http_qps'].max():.2f}")

            # å–å¾—å‰ 20 ç­†çš„æ™‚é–“æˆ³è¨˜
            filtered_timestamps = df_top20['timestamp'].tolist()
            # æŒ‰æ™‚é–“æ’åºï¼ˆæ–¹ä¾¿é–±è®€ï¼‰
            filtered_timestamps.sort()

            # ä¿®æ”¹ï¼šåŒ¯å‡ºåˆ°å›ºå®šæª”åï¼Œä¸å«æ—¥æœŸæ™‚é–“
            top20_output_file = str(TEST_FILE_DIR / "control_group_http_qps_top20.csv")
            print()
            print(f"ğŸ’¾ åŒ¯å‡º Top 20 è³‡æ–™åˆ°å›ºå®šæª”å: {top20_output_file}")

            with open(top20_output_file, 'w', newline='', encoding='utf-8-sig') as csvfile:
                # æº–å‚™æ¬„ä½åç¨± (èˆ‡åŸå§‹æª”æ¡ˆç›¸åŒ)
                fieldnames = ['timestamp'] + [
                    f"{metric['name']} ({metric['description']})"
                    for metric in queries
                ]

                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writeheader()

                # å¯«å…¥ç¯©é¸å¾Œçš„è³‡æ–™
                for ts in filtered_timestamps:
                    row = {'timestamp': ts.strftime('%Y-%m-%d %H:%M:%S')}

                    for metric in queries:
                        metric_name = metric['name']
                        column_name = f"{metric_name} ({metric['description']})"

                        # å–å¾—è©²æ™‚é–“é»çš„å€¼ï¼Œå¦‚æœæ²’æœ‰å‰‡ç•™ç©º
                        value = all_data[metric_name]['data'].get(ts, '')
                        row[column_name] = value

                    writer.writerow(row)

            print(f"âœ… Top 20 è³‡æ–™å·²åŒ¯å‡ºåˆ°å›ºå®šæª”æ¡ˆ!")
            print(f"   æª”æ¡ˆ: {top20_output_file}")
            print(f"   è³‡æ–™ç­†æ•¸: {len(filtered_timestamps)}")
            if filtered_timestamps:
                print(f"   æ™‚é–“ç¯„åœ: {filtered_timestamps[0]} ~ {filtered_timestamps[-1]}")

            # é¡¯ç¤ºç¯©é¸å¾Œçš„çµ±è¨ˆæ‘˜è¦
            print()
            print("ğŸ“Š Top 20 çµ±è¨ˆæ‘˜è¦:")
            for metric in queries:
                metric_name = metric['name']
                # åªå–ç¯©é¸å¾Œæ™‚é–“æˆ³è¨˜çš„è³‡æ–™
                filtered_metric_values = [
                    all_data[metric_name]['data'].get(ts, 0)
                    for ts in filtered_timestamps
                    if all_data[metric_name]['data'].get(ts) is not None
                ]

                if filtered_metric_values:
                    print(f"   {metric['description']}:")
                    print(f"      å¹³å‡å€¼: {sum(filtered_metric_values)/len(filtered_metric_values):.2f}")
                    print(f"      æœ€å¤§å€¼: {max(filtered_metric_values):.2f}")
                    print(f"      æœ€å°å€¼: {min(filtered_metric_values):.2f}")
        else:
                        print("âš ï¸  ç„¡æ³•é€²è¡Œç¯©é¸: 2_http_qps è³‡æ–™ä¸å­˜åœ¨")

    def filter_logs_per_second_by_median(self, csv_file: str, output_file: str = None):
        """
        ä½¿ç”¨ pandas å° CSV æ–‡ä»¶ä¸­çš„ 'logs per second' æ¬„ä½é€²è¡Œä½åå·®å€¼ç¯©é¸
        ç¯©é¸æ¢ä»¶ï¼šå…ˆæ’é™¤ 0ã€ç©ºå€¼ã€null å€¼ï¼Œç„¶å¾Œä»¥å¤§æ–¼ä¸­ä½æ•¸çš„æ•¸æ“šé€²è¡Œç¯©é¸
        ç¯©é¸å¾Œè¨ˆç®—å¹³å‡å€¼

        Args:
            csv_file: è¼¸å…¥çš„ CSV æª”æ¡ˆè·¯å¾‘
            output_file: è¼¸å‡ºçš„ CSV æª”æ¡ˆè·¯å¾‘ï¼ˆå¯é¸ï¼Œé è¨­ç‚ºåŸæª”æ¡ˆååŠ ä¸Š '_filtered'ï¼‰
        """
        import pandas as pd

        if output_file is None:
            # ç”Ÿæˆè¼¸å‡ºæª”åï¼šåŸæª”å + '_filtered'
            import os
            base_name = os.path.splitext(csv_file)[0]
            extension = os.path.splitext(csv_file)[1]
            output_file = f"{base_name}_filtered{extension}"

        try:
            # è®€å– CSV æ–‡ä»¶
            print(f"\nğŸ“Š ä½¿ç”¨ pandas é€²è¡Œä½åå·®å€¼ç¯©é¸åˆ†æ...")
            print(f"   è®€å–æª”æ¡ˆ: {csv_file}")

            df = pd.read_csv(csv_file)
            print(f"   åŸå§‹è³‡æ–™ç­†æ•¸: {len(df)}")

            # å°‹æ‰¾åŒ…å« "logs" å’Œ "per" å’Œ "second" çš„æ¬„ä½
            logs_column = None
            for col in df.columns:
                if 'logs' in col.lower() and 'per' in col.lower() and 'second' in col.lower():
                    logs_column = col
                    break

            if logs_column is None:
                print("âŒ æ‰¾ä¸åˆ°åŒ…å« 'logs per second' çš„æ¬„ä½")
                print(f"   å¯ç”¨æ¬„ä½: {list(df.columns)}")
                return None

            print(f"   ç›®æ¨™æ¬„ä½: '{logs_column}'")

            # ä¿®æ”¹ï¼šå…ˆç§»é™¤ NaN å€¼ã€ç©ºå€¼å’Œéæ•¸å€¼è³‡æ–™
            original_count = len(df)
            df_clean = df[pd.notna(df[logs_column]) & (df[logs_column] != '')]

            # ç¢ºä¿è©²æ¬„ä½ç‚ºæ•¸å€¼å‹æ…‹
            df_clean.loc[:, logs_column] = pd.to_numeric(df_clean[logs_column], errors='coerce')
            df_clean = df_clean.dropna(subset=[logs_column])

            # ä¿®æ”¹ï¼šå†ç§»é™¤ 0 å€¼
            df_clean = df_clean[df_clean[logs_column] > 0]

            print(f"   æ¸…ç†å¾Œè³‡æ–™ç­†æ•¸: {len(df_clean)} (ç§»é™¤ {original_count - len(df_clean)} ç­†ç„¡æ•ˆè³‡æ–™ï¼ŒåŒ…å« 0ã€ç©ºå€¼ã€null)")
            # print(f"   æ¸…ç†å¾Œè³‡æ–™ç­†æ•¸: {len(df_clean)} (ç§»é™¤ {original_count - len(df_clean)} ç­†ç„¡æ•ˆè³‡æ–™)")

            if len(df_clean) == 0:
                print("âŒ æ¸…ç†å¾Œæ²’æœ‰æœ‰æ•ˆè³‡æ–™")
                return None

            # è¨ˆç®—çµ±è¨ˆæ•¸æ“š
            logs_data = df_clean[logs_column]

            print(f"\nğŸ“ˆ '{logs_column}' çµ±è¨ˆåˆ†æ (å·²æ’é™¤ 0ã€ç©ºå€¼ã€null):")
            # print(f"\nğŸ“ˆ '{logs_column}' çµ±è¨ˆåˆ†æ:")
            print(f"   å¹³å‡å€¼: {logs_data.mean():.2f}")
            print(f"   ä¸­ä½æ•¸: {logs_data.median():.2f}")
            print(f"   æ¨™æº–å·®: {logs_data.std():.2f}")
            print(f"   æœ€å°å€¼: {logs_data.min():.2f}")
            print(f"   æœ€å¤§å€¼: {logs_data.max():.2f}")
            # print(f"   éé›¶å€¼æ•¸é‡: {(logs_data > 0).sum()}")
            # print(f"   é›¶å€¼æ•¸é‡: {(logs_data == 0).sum()}")
            print(f"   æœ‰æ•ˆå€¼æ•¸é‡: {len(logs_data)}")

            # è¨ˆç®—ä¸­ä½æ•¸
            median_value = logs_data.median()
            print(f"\nğŸ¯ ç¯©é¸æ¢ä»¶: > {median_value:.2f} (ä¸­ä½æ•¸)")

            # é€²è¡Œç¯©é¸ï¼šå¤§æ–¼ä¸­ä½æ•¸çš„è³‡æ–™
            filtered_df = df_clean[df_clean[logs_column] > median_value].copy()

            print(f"   ç¯©é¸å‰è³‡æ–™ç­†æ•¸: {len(df_clean)}")
            print(f"   ç¯©é¸å¾Œè³‡æ–™ç­†æ•¸: {len(filtered_df)}")
            print(f"   ä¿ç•™æ¯”ä¾‹: {len(filtered_df)/len(df_clean)*100:.1f}%")

            if len(filtered_df) == 0:
                print("âš ï¸  ç¯©é¸å¾Œæ²’æœ‰è³‡æ–™ï¼ˆæ‰€æœ‰å€¼éƒ½å°æ–¼ç­‰æ–¼ä¸­ä½æ•¸ï¼‰")
                return None

            # è¨ˆç®—ç¯©é¸å¾Œçš„çµ±è¨ˆæ•¸æ“š
            filtered_logs_data = filtered_df[logs_column]

            print(f"\nğŸ“Š ç¯©é¸å¾Œçµ±è¨ˆ:")
            print(f"   å¹³å‡å€¼: {filtered_logs_data.mean():.2f}")
            print(f"   ä¸­ä½æ•¸: {filtered_logs_data.median():.2f}")
            print(f"   æ¨™æº–å·®: {filtered_logs_data.std():.2f}")
            print(f"   æœ€å°å€¼: {filtered_logs_data.min():.2f}")
            print(f"   æœ€å¤§å€¼: {filtered_logs_data.max():.2f}")

            # èˆ‡åŸå§‹æ•¸æ“šæ¯”è¼ƒ
            print(f"\nğŸ“ˆ ç¯©é¸æ•ˆæœ:")
            print(f"   å¹³å‡å€¼è®ŠåŒ–: {logs_data.mean():.2f} â†’ {filtered_logs_data.mean():.2f} (æå‡ {(filtered_logs_data.mean()-logs_data.mean())/logs_data.mean()*100:.1f}%)")
            print(f"   æ¨™æº–å·®è®ŠåŒ–: {logs_data.std():.2f} â†’ {filtered_logs_data.std():.2f}")

            # è¨ˆç®—è®Šç•°ä¿‚æ•¸æ”¹å–„
            original_cv = logs_data.std() / logs_data.mean() if logs_data.mean() > 0 else 0
            filtered_cv = filtered_logs_data.std() / filtered_logs_data.mean() if filtered_logs_data.mean() > 0 else 0
            print(f"   è®Šç•°ä¿‚æ•¸: {original_cv:.3f} â†’ {filtered_cv:.3f}")

            # å„²å­˜ç¯©é¸å¾Œçš„è³‡æ–™
            print(f"\nğŸ’¾ å„²å­˜ç¯©é¸å¾Œè³‡æ–™...")
            print(f"   è¼¸å‡ºæª”æ¡ˆ: {output_file}")

            filtered_df.to_csv(output_file, index=False, encoding='utf-8-sig')

            print(f"âœ… ç¯©é¸å®Œæˆ!")
            print(f"   åŸå§‹è³‡æ–™: {len(df_clean)} ç­†")
            print(f"   ç¯©é¸å¾Œ: {len(filtered_df)} ç­† (ä¿ç•™ {len(filtered_df)/len(df_clean)*100:.1f}%)")
            print(f"   ç¯©é¸å¾Œå¹³å‡å€¼: {filtered_logs_data.mean():.2f}")

            return {
                'original_count': len(df_clean),
                'filtered_count': len(filtered_df),
                'original_mean': logs_data.mean(),
                'filtered_mean': filtered_logs_data.mean(),
                'median_threshold': median_value,
                'output_file': output_file
            }

        except ImportError:
            print("âŒ pandas å¥—ä»¶æœªå®‰è£")
            print("   è«‹åŸ·è¡Œ: pip install pandas")
            return None
        except FileNotFoundError:
            print(f"âŒ æª”æ¡ˆä¸å­˜åœ¨: {csv_file}")
            return None
        except Exception as e:
            print(f"âŒ ç¯©é¸éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
            return None

    def filter_http_qps_top20(self, csv_file: str) -> str:
        """
        æ–°å¢åŠŸèƒ½ï¼šç¯©é¸ HTTP QPS æ¬„ä½ï¼Œæ’é™¤ 0ã€ç©ºå€¼å’Œ nullï¼Œ
        æŒ‰ç…§é™åºæ’åºï¼Œå–å‰ 20 ç­†ï¼ŒåŒ¯å‡ºåˆ°å›ºå®šæª”åçš„ CSV

        Args:
            csv_file: è¼¸å…¥çš„ CSV æª”æ¡ˆè·¯å¾‘

        Returns:
            è¼¸å‡ºæª”æ¡ˆè·¯å¾‘
        """
        import pandas as pd

        print()
        print("=" * 70)
        print("  ğŸ” å°ç…§çµ„ HTTP QPS Top 20 åˆ†æ")
        print("=" * 70)
        print(f"   è®€å–æª”æ¡ˆ: {csv_file}")

        try:
            # è®€å– CSV
            df = pd.read_csv(csv_file)
            print(f"   åŸå§‹è³‡æ–™ç­†æ•¸: {len(df)}")

            # å°‹æ‰¾ HTTP QPS æ¬„ä½
            http_qps_column = None
            for col in df.columns:
                if '2_http_qps' in col:
                    http_qps_column = col
                    break

            if http_qps_column is None:
                print("âŒ æ‰¾ä¸åˆ° '2_http_qps' æ¬„ä½")
                print(f"   å¯ç”¨æ¬„ä½: {list(df.columns)}")
                return None

            print(f"   ç›®æ¨™æ¬„ä½: '{http_qps_column}'")

            # ç¯©é¸æ‰ 0ã€ç©ºå€¼å’Œ null
            df_clean = df.copy()
            df_clean = df_clean[pd.notna(df_clean[http_qps_column]) & (df_clean[http_qps_column] != '')]
            df_clean.loc[:, http_qps_column] = pd.to_numeric(df_clean[http_qps_column], errors='coerce')
            df_clean = df_clean.dropna(subset=[http_qps_column])
            df_clean = df_clean[df_clean[http_qps_column] > 0]

            print(f"   ç¯©é¸å¾Œè³‡æ–™ç­†æ•¸: {len(df_clean)} (ç§»é™¤äº† {len(df) - len(df_clean)} ç­†ç„¡æ•ˆè³‡æ–™)")

            if len(df_clean) == 0:
                print("âŒ ç¯©é¸å¾Œæ²’æœ‰æœ‰æ•ˆè³‡æ–™")
                return None

            # é™åºæ’åºä¸¦å–å‰ 20 ç­†
            df_sorted = df_clean.sort_values(by=http_qps_column, ascending=False)
            df_top20 = df_sorted.head(20)

            print(f"   å–å¾—å‰ 20 ç­†è³‡æ–™")

            # ç¢ºå®šè¼¸å‡ºæª”æ¡ˆè·¯å¾‘ï¼ˆå›ºå®šæª”åï¼Œæ”¾åœ¨ test_file/ ç›®éŒ„ï¼‰
            test_file_dir = TEST_FILE_DIR
            output_file = str(test_file_dir / "control_group_http_qps_top20.csv")

            # åŒ¯å‡º CSV
            df_top20.to_csv(output_file, index=False, encoding='utf-8-sig')

            print()
            print(f"âœ… åŒ¯å‡ºå®Œæˆ!")
            print(f"   è¼¸å‡ºæª”æ¡ˆ: {output_file}")
            print(f"   è³‡æ–™ç­†æ•¸: {len(df_top20)}")
            print()
            print("ğŸ“Š Top 20 çµ±è¨ˆæ‘˜è¦:")
            print(f"   æœ€å¤§å€¼: {df_top20[http_qps_column].max():.2f}")
            print(f"   æœ€å°å€¼: {df_top20[http_qps_column].min():.2f}")
            print(f"   å¹³å‡å€¼: {df_top20[http_qps_column].mean():.2f}")
            print(f"   ä¸­ä½æ•¸: {df_top20[http_qps_column].median():.2f}")
            print("=" * 70)

            return output_file

        except Exception as e:
            print(f"âŒ è™•ç†å¤±æ•—: {e}")
            import traceback
            traceback.print_exc()
            return None

# ==========================================
# ç”Ÿæˆæ¸¬è©¦è³‡æ–™
# ==========================================
def generate_log_data(device_id: str, log_num: int) -> dict:
    """ç”Ÿæˆéš¨æ©Ÿæ—¥èªŒè³‡æ–™"""
    log_level = random.choice(LOG_LEVELS)
    message_template = random.choice(LOG_MESSAGES)

    if "{usage}" in message_template:
        message = message_template.format(usage=random.randint(50, 95))
    elif "{temp}" in message_template:
        message = message_template.format(temp=random.randint(40, 85))
    else:
        message = message_template

    return {
        "device_id": device_id,
        "log_level": log_level,
        "message": f"{message} (#{log_num})",
        "log_data": {
            "test_id": log_num,
            "timestamp": datetime.now().isoformat(),
            "random_value": random.random(),
            "sequence": log_num
        }
    }

# ==========================================
# ç™¼é€å–®ç­†æ—¥èªŒ
# ==========================================
async def send_log(session: aiohttp.ClientSession, device_id: str, log_num: int) -> dict:
    """ç™¼é€å–®ç­†æ—¥èªŒåˆ° API"""
    url = f"{BASE_URL}/api/log"
    log_data = generate_log_data(device_id, log_num)

    start_time = time.time()

    try:
        async with session.post(url, json=log_data, timeout=aiohttp.ClientTimeout(total=30)) as response:
            response_time = (time.time() - start_time) * 1000

            if response.status == 200:
                return {
                    "success": True,
                    "response_time": response_time,
                    "status": response.status,
                    "error": None,
                    "count": 1
                }
            else:
                return {
                    "success": False,
                    "response_time": response_time,
                    "status": response.status,
                    "error": await response.text(),
                    "count": 1
                }

    except asyncio.TimeoutError:
        return {
            "success": False,
            "response_time": (time.time() - start_time) * 1000,
            "status": 0,
            "error": "è«‹æ±‚è¶…æ™‚",
            "count": 1
        }
    except Exception as e:
        return {
            "success": False,
            "response_time": (time.time() - start_time) * 1000,
            "status": 0,
            "error": str(e),
            "count": 1
        }

# ==========================================
# ç™¼é€æ‰¹é‡æ—¥èªŒ
# ==========================================
async def send_batch_logs(session: aiohttp.ClientSession, logs: List[dict]) -> dict:
    """æ‰¹é‡ç™¼é€æ—¥èªŒåˆ° API"""
    url = f"{BASE_URL}/api/logs/batch"
    batch_data = {"logs": logs}

    start_time = time.time()

    try:
        async with session.post(url, json=batch_data, timeout=aiohttp.ClientTimeout(total=60)) as response:
            response_time = (time.time() - start_time) * 1000

            if response.status == 200:
                return {
                    "success": True,
                    "response_time": response_time,
                    "status": response.status,
                    "error": None,
                    "count": len(logs)
                }
            else:
                return {
                    "success": False,
                    "response_time": response_time,
                    "status": response.status,
                    "error": await response.text(),
                    "count": len(logs)
                }

    except asyncio.TimeoutError:
        return {
            "success": False,
            "response_time": (time.time() - start_time) * 1000,
            "status": 0,
            "error": "è«‹æ±‚è¶…æ™‚",
            "count": len(logs)
        }
    except Exception as e:
        return {
            "success": False,
            "response_time": (time.time() - start_time) * 1000,
            "status": 0,
            "error": str(e),
            "count": len(logs)
        }

# ==========================================
# æ‰¹æ¬¡ç™¼é€æ—¥èªŒ
# ==========================================
async def batch_send_logs(
    session: aiohttp.ClientSession,
    device_id: str,
    num_logs: int,
    semaphore: asyncio.Semaphore
) -> List[dict]:
    """æ‰¹æ¬¡ç™¼é€æ—¥èªŒï¼ˆä½¿ç”¨ä¿¡è™Ÿé‡æ§åˆ¶ä¸¦ç™¼ï¼‰"""
    if USE_BATCH_API:
        all_logs = [generate_log_data(device_id, log_num) for log_num in range(num_logs)]
        results = []

        for i in range(0, len(all_logs), BATCH_SIZE):
            batch = all_logs[i:i + BATCH_SIZE]
            async with semaphore:
                result = await send_batch_logs(session, batch)
                results.append(result)

        return results
    else:
        async def send_with_semaphore(log_num: int) -> dict:
            async with semaphore:
                return await send_log(session, device_id, log_num)

        tasks = [send_with_semaphore(log_num) for log_num in range(num_logs)]
        return await asyncio.gather(*tasks)

# ==========================================
# ä¸»è¦å£“åŠ›æ¸¬è©¦
# ==========================================
async def stress_test(
    num_devices: int = NUM_DEVICES,
    logs_per_device: int = LOGS_PER_DEVICE,
    concurrent_limit: int = CONCURRENT_LIMIT,
    iteration: int = 1,
    current_iteration: int = 1
):
    """åŸ·è¡Œå£“åŠ›æ¸¬è©¦"""
    print("=" * 70)
    if iteration > 1:
        print(f"  ğŸ“Š å°ç…§çµ„ - ç°¡åŒ–ç³»çµ±å£“åŠ›æ¸¬è©¦ [ç¬¬ {current_iteration}/{iteration} è¼ª]")
    else:
        print("  ğŸ“Š å°ç…§çµ„ - ç°¡åŒ–ç³»çµ±å£“åŠ›æ¸¬è©¦")
    print("=" * 70)
    print(f"æ¸¬è©¦é…ç½®ï¼š")
    print(f"  â€¢ è¨­å‚™æ•¸é‡: {num_devices}")
    print(f"  â€¢ æ¯å°è¨­å‚™æ—¥èªŒæ•¸: {logs_per_device}")
    print(f"  â€¢ ç¸½æ—¥èªŒæ•¸: {num_devices * logs_per_device:,}")
    print(f"  â€¢ ä¸¦ç™¼é™åˆ¶: {concurrent_limit}")
    print(f"  â€¢ API ç«¯é»: {BASE_URL}")
    print(f"  â€¢ ç³»çµ±ç‰¹æ€§: ç„¡ Nginxã€é€£æ¥æ± ã€Redisã€Worker")
    if iteration > 1:
        print(f"  â€¢ ç¸½å¾ªç’°æ¬¡æ•¸: {iteration}")
        print(f"  â€¢ ç•¶å‰å¾ªç’°: {current_iteration}")
    print("-" * 70)

    semaphore = asyncio.Semaphore(concurrent_limit)

    # è¨˜éŒ„æ¸¬è©¦é–‹å§‹æ™‚é–“ï¼ˆç”¨æ–¼ Prometheus æŸ¥è©¢ï¼‰
    test_start_datetime = datetime.now()
    start_time = time.time()

    connector = aiohttp.TCPConnector(limit=concurrent_limit, limit_per_host=concurrent_limit)
    timeout = aiohttp.ClientTimeout(total=600)  # 10åˆ†é˜è¶…æ™‚ï¼ˆç°¡åŒ–ç‰ˆè¼ƒæ…¢ï¼‰

    async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
        device_tasks = []

        for device_num in range(num_devices):
            # ä¿®æ”¹ï¼šåŠ å…¥ 'control_' å‰ç¶´ä»¥å€åˆ†å°ç…§çµ„æ¸¬è©¦è³‡æ–™
            device_id = f"control_device_{device_num:03d}"
            task = batch_send_logs(session, device_id, logs_per_device, semaphore)
            device_tasks.append(task)

        print("â³ é–‹å§‹ç™¼é€æ—¥èªŒ...")
        all_results = await asyncio.gather(*device_tasks)

    total_time = time.time() - start_time
    # è¨˜éŒ„æ¸¬è©¦çµæŸæ™‚é–“ï¼ˆç”¨æ–¼ Prometheus æŸ¥è©¢ï¼‰
    test_end_datetime = datetime.now()

    # æ•´ç†çµæœ
    all_responses = [result for device_results in all_results for result in device_results]

    total_requests = len(all_responses)
    successful_requests = sum(1 for r in all_responses if r["success"])
    failed_requests = total_requests - successful_requests
    total_logs_sent = sum(r.get("count", 1) for r in all_responses)
    successful_logs = sum(r.get("count", 1) for r in all_responses if r["success"])

    response_times = [r["response_time"] for r in all_responses if r["success"]]

    if response_times:
        avg_response_time = sum(response_times) / len(response_times)
        min_response_time = min(response_times)
        max_response_time = max(response_times)

        sorted_times = sorted(response_times)
        p50 = sorted_times[int(len(sorted_times) * 0.50)]
        p95 = sorted_times[int(len(sorted_times) * 0.95)]
        p99 = sorted_times[int(len(sorted_times) * 0.99)]
    else:
        avg_response_time = 0
        min_response_time = 0
        max_response_time = 0
        p50 = p95 = p99 = 0

    throughput = successful_logs / total_time if total_time > 0 else 0

    # è¨ˆç®— QPSï¼ˆè«‹æ±‚æ•¸/ç§’ï¼‰
    qps = successful_requests / total_time if total_time > 0 else 0

    # è¼¸å‡ºçµæœ
    print("\n" + "=" * 70)
    print("  ğŸ“ˆ æ¸¬è©¦çµæœ")
    print("=" * 70)

    print(f"\nâ±ï¸  æ™‚é–“çµ±è¨ˆï¼š")
    print(f"  â€¢ ç¸½è€—æ™‚: {total_time:.2f} ç§’")

    print(f"\nğŸ“Š è«‹æ±‚çµ±è¨ˆï¼š")
    if USE_BATCH_API:
        print(f"  â€¢ æ‰¹é‡è«‹æ±‚æ•¸: {total_requests:,}")
        print(f"  â€¢ ç¸½æ—¥èªŒæ•¸: {total_logs_sent:,}")
        print(f"  â€¢ æˆåŠŸæ—¥èªŒ: {successful_logs:,} ({successful_logs/total_logs_sent*100:.1f}%)")
    else:
        print(f"  â€¢ ç¸½è«‹æ±‚æ•¸: {total_requests:,}")
    print(f"  â€¢ æˆåŠŸè«‹æ±‚: {successful_requests:,} ({successful_requests/total_requests*100:.1f}%)")
    print(f"  â€¢ å¤±æ•—è«‹æ±‚: {failed_requests:,} ({failed_requests/total_requests*100:.1f}%)")

    print(f"\nâš¡ æ•ˆèƒ½æŒ‡æ¨™ï¼š")
    print(f"  â€¢ QPS: {qps:.2f} req/ç§’")
    print(f"  â€¢ ååé‡: {throughput:.2f} logs/ç§’")
    print(f"  â€¢ å¹³å‡å›æ‡‰æ™‚é–“: {avg_response_time:.2f} ms")
    print(f"  â€¢ æœ€å°å›æ‡‰æ™‚é–“: {min_response_time:.2f} ms")
    print(f"  â€¢ æœ€å¤§å›æ‡‰æ™‚é–“: {max_response_time:.2f} ms")

    print(f"\nğŸ“‰ ç™¾åˆ†ä½æ•¸ï¼š")
    print(f"  â€¢ P50 (ä¸­ä½æ•¸): {p50:.2f} ms")
    print(f"  â€¢ P95: {p95:.2f} ms")
    print(f"  â€¢ P99: {p99:.2f} ms")

    # éŒ¯èª¤åˆ†æï¼ˆç”¨æ–¼ JSON åŒ¯å‡ºå’Œæ§åˆ¶å°é¡¯ç¤ºï¼‰
    error_types = {}
    if failed_requests > 0:
        print(f"\nâŒ éŒ¯èª¤åˆ†æï¼š")
        for r in all_responses:
            if not r["success"]:
                error = r["error"] or f"HTTP {r['status']}"
                error_types[error] = error_types.get(error, 0) + 1

        for error, count in sorted(error_types.items(), key=lambda x: x[1], reverse=True):
            print(f"  â€¢ {error}: {count} æ¬¡")

    print("\n" + "=" * 70)

    target_throughput = 10000
    target_p95 = 100

    print(f"\nğŸ¯ ç›®æ¨™é”æˆæƒ…æ³ï¼š")

    if throughput >= target_throughput:
        print(f"  âœ… ååé‡é”æ¨™: {throughput:.2f} >= {target_throughput} logs/ç§’")
    else:
        print(f"  âŒ ååé‡æœªé”æ¨™: {throughput:.2f} < {target_throughput} logs/ç§’")

    if p95 <= target_p95:
        print(f"  âœ… P95 å›æ‡‰æ™‚é–“é”æ¨™: {p95:.2f} <= {target_p95} ms")
    else:
        print(f"  âŒ P95 å›æ‡‰æ™‚é–“æœªé”æ¨™: {p95:.2f} > {target_p95} ms")

    if failed_requests == 0:
        print(f"  âœ… ç„¡å¤±æ•—è«‹æ±‚")
    else:
        print(f"  âš ï¸ æœ‰ {failed_requests} å€‹å¤±æ•—è«‹æ±‚")

    print("=" * 70)

    # ==========================================
    # åŒ¯å‡º Prometheus æŒ‡æ¨™
    # ==========================================
    # ä¿®æ”¹ï¼šåªåœ¨æœ€å¾Œä¸€è¼ªæ¸¬è©¦å®Œæˆæ™‚æ‰åŒ¯å‡ºæŒ‡æ¨™ï¼ˆæ•´åˆæ‰€æœ‰æ¸¬è©¦æ•¸æ“šï¼‰
    # if EXPORT_METRICS:
    #     try:
    #         print("\n" + "=" * 70)
    #         print("  ğŸ“Š åŒ¯å‡º Prometheus ååé‡æŒ‡æ¨™")
    #         print("=" * 70)
    #
    #         exporter = PrometheusExporter(PROMETHEUS_URL)
    #
    #         # ç‚ºæ¯å€‹å¾ªç’°ç”Ÿæˆå”¯ä¸€çš„æª”æ¡ˆåç¨±
    #         if iteration > 1:
    #             output_file = f"control_group_throughput_metrics_iter{current_iteration:02d}.csv"
    #         else:
    #             output_file = METRICS_OUTPUT_FILE
    #
    #         exporter.export_throughput_metrics(
    #             test_start_datetime,
    #             test_end_datetime,
    #             output_file
    #         )
    #
    #         print("=" * 70)
    #     except Exception as e:
    #         print(f"\nâš ï¸  æŒ‡æ¨™åŒ¯å‡ºå¤±æ•—: {e}")
    #         print("   æ¸¬è©¦çµæœä¸å—å½±éŸ¿ï¼Œå¯æ‰‹å‹•åŒ¯å‡ºæŒ‡æ¨™")

    # ä¿®æ”¹ï¼šè¿”å›å®Œæ•´æ¸¬è©¦çµæœä¾› JSON åŒ¯å‡ºä½¿ç”¨ï¼ˆåƒè€ƒ tests/stress_test.pyï¼‰
    return {
        "iteration": current_iteration,
        "total_iterations": iteration,
        "timestamp": datetime.now().isoformat(),
        "test_time_range": {
            "start": test_start_datetime.isoformat(),
            "end": test_end_datetime.isoformat()
        },
        "config": {
            "num_devices": num_devices,
            "logs_per_device": logs_per_device,
            "total_logs": num_devices * logs_per_device,
            "concurrent_limit": concurrent_limit,
            "batch_size": BATCH_SIZE,
            "use_batch_api": USE_BATCH_API,
            "base_url": BASE_URL
        },
        "timing": {
            "total_time": round(total_time, 2),
        },
        "requests": {
            "total_requests": total_requests,
            "successful_requests": successful_requests,
            "failed_requests": failed_requests,
            "success_rate": round(successful_requests/total_requests*100, 2) if total_requests > 0 else 0
        },
        "logs": {
            "total_logs_sent": total_logs_sent,
            "successful_logs": successful_logs,
            "success_rate": round(successful_logs/total_logs_sent*100, 2) if total_logs_sent > 0 else 0
        },
        "performance": {
            "qps": round(qps, 2),
            "throughput": round(throughput, 2),
            "avg_response_time": round(avg_response_time, 2),
            "min_response_time": round(min_response_time, 2),
            "max_response_time": round(max_response_time, 2)
        },
        "percentiles": {
            "p50": round(p50, 2),
            "p95": round(p95, 2),
            "p99": round(p99, 2)
        },
        "errors": error_types,
        "targets": {
            "throughput": {
                "target": target_throughput,
                "actual": round(throughput, 2),
                "achieved": throughput >= target_throughput
            },
            "p95_response_time": {
                "target": target_p95,
                "actual": round(p95, 2),
                "achieved": p95 <= target_p95
            },
            "zero_failures": {
                "achieved": failed_requests == 0,
                "failed_count": failed_requests
            }
        }
    }

# ==========================================
# ä¸»ç¨‹å¼
# ==========================================
async def main():
    """ä¸»ç¨‹å¼å…¥å£"""
    # è¨˜éŒ„æ•´é«”æ¸¬è©¦é–‹å§‹æ™‚é–“ï¼ˆç”¨æ–¼åŒ¯å‡ºæŒ‡æ¨™å’Œ JSONï¼‰
    overall_start_time = datetime.now()

    # ä¿®æ”¹ï¼šè¨˜éŒ„æ‰€æœ‰æ¸¬è©¦çš„æ™‚é–“ç¯„åœï¼ˆç”¨æ–¼ Prometheus æŒ‡æ¨™åŒ¯å‡ºï¼‰
    all_test_start = None
    all_test_end = None

    # æ–°å¢ï¼šæ”¶é›†æ‰€æœ‰æ¸¬è©¦çµæœï¼ˆç”¨æ–¼ JSON åŒ¯å‡ºï¼‰
    all_test_results = []

    for i in range(NUM_ITERATIONS):
        # ä¿®æ”¹ï¼šæ¥æ”¶æ¸¬è©¦çµæœå­—å…¸è€Œéæ™‚é–“å…ƒçµ„
        result = await stress_test(
            num_devices=NUM_DEVICES,
            logs_per_device=LOGS_PER_DEVICE,
            concurrent_limit=CONCURRENT_LIMIT,
            iteration=NUM_ITERATIONS,
            current_iteration=i + 1
        )

        # æ”¶é›†æ¸¬è©¦çµæœ
        all_test_results.append(result)

        # å¾çµæœä¸­æå–æ™‚é–“ç¯„åœç”¨æ–¼ Prometheus åŒ¯å‡º
        test_start = datetime.fromisoformat(result["test_time_range"]["start"])
        test_end = datetime.fromisoformat(result["test_time_range"]["end"])

        # è¨˜éŒ„ç¬¬ä¸€æ¬¡æ¸¬è©¦çš„é–‹å§‹æ™‚é–“
        if all_test_start is None:
            all_test_start = test_start

        # æ›´æ–°æœ€å¾Œä¸€æ¬¡æ¸¬è©¦çš„çµæŸæ™‚é–“
        all_test_end = test_end

        # ç°¡å–®é¡¯ç¤ºé€²åº¦
        print(f"âœ… ç¬¬ {i + 1}/{NUM_ITERATIONS} è¼ªæ¸¬è©¦å®Œæˆ")

        if i < NUM_ITERATIONS - 1 and ITERATION_INTERVAL > 0:
            print(f"\nâ¸ï¸  ç­‰å¾… {ITERATION_INTERVAL} ç§’å¾Œé–‹å§‹ä¸‹ä¸€è¼ªæ¸¬è©¦...")
            await asyncio.sleep(ITERATION_INTERVAL)

    # è¨˜éŒ„æ•´é«”æ¸¬è©¦çµæŸæ™‚é–“
    overall_end_time = datetime.now()

    # ==========================================
    # æ–°å¢ï¼šæŸ¥è©¢ Prometheus æŒ‡æ¨™ï¼ˆåƒè€ƒ tests/stress_test.pyï¼‰
    # ==========================================
    print("\n" + "=" * 70)
    print("â³ ç­‰å¾… 10 ç§’è®“ Prometheus æ”¶é›†å®Œæ•´æŒ‡æ¨™...")
    print("=" * 70)
    await asyncio.sleep(10)

    # ä¿®æ”¹ï¼šæ‰€æœ‰æ¸¬è©¦å®Œæˆå¾Œï¼ŒåŒ¯å‡ºæ•´åˆçš„ Prometheus æŒ‡æ¨™åˆ°å–®ä¸€ CSV æª”æ¡ˆ
    if EXPORT_METRICS and all_test_start and all_test_end:
        try:
            print("\n" + "=" * 70)
            print("  ğŸ“Š åŒ¯å‡ºæ‰€æœ‰æ¸¬è©¦çš„ Prometheus ååé‡æŒ‡æ¨™ï¼ˆæ•´åˆç‰ˆï¼‰")
            print("=" * 70)
            print(f"  â€¢ æ¸¬è©¦è¼ªæ•¸: {NUM_ITERATIONS}")
            print(f"  â€¢ ç¸½æ™‚é–“ç¯„åœ: {all_test_start} ~ {all_test_end}")
            print(f"  â€¢ è¼¸å‡ºæª”æ¡ˆ: {METRICS_OUTPUT_FILE}")
            print("=" * 70)

            exporter = PrometheusExporter(PROMETHEUS_URL)
            exporter.export_throughput_metrics(
                all_test_start,
                all_test_end,
                METRICS_OUTPUT_FILE
            )

            print("=" * 70)
        except Exception as e:
            print(f"\nâš ï¸  æŒ‡æ¨™åŒ¯å‡ºå¤±æ•—: {e}")
            print("   æ¸¬è©¦çµæœä¸å—å½±éŸ¿ï¼Œå¯æ‰‹å‹•åŒ¯å‡ºæŒ‡æ¨™")


    # ==========================================
    # æ–°å¢ï¼šåŒ¯å‡ºæ‰€æœ‰æ¸¬è©¦çµæœç‚º JSONï¼ˆåŒ…å« Prometheus æŒ‡æ¨™ï¼‰
    # ==========================================
    print("\n" + "=" * 70)
    print("  ğŸ“„ åŒ¯å‡ºå°ç…§çµ„æ¸¬è©¦çµæœ")
    print("=" * 70)

    # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
    TEST_FILE_DIR.mkdir(parents=True, exist_ok=True)

    # ç”¢ç”Ÿè¼¸å‡ºæª”æ¡ˆåç¨±ï¼ˆä½¿ç”¨æ™‚é–“æˆ³è¨˜ï¼‰
    timestamp_str = overall_start_time.strftime("%Y%m%d_%H%M%S")
    output_file = TEST_FILE_DIR / f"control_group_stress_test_results_{timestamp_str}.json"

    # æº–å‚™å®Œæ•´çš„æ¸¬è©¦å ±å‘Š
    test_report = {
        "test_summary": {
            "test_type": "control_group",
            "start_time": overall_start_time.isoformat(),
            "end_time": overall_end_time.isoformat(),
            "total_duration": round((overall_end_time - overall_start_time).total_seconds(), 2),
            "num_iterations": NUM_ITERATIONS,
            "iteration_interval": ITERATION_INTERVAL
        },
        "iterations": all_test_results
    }

    # åŒ¯å‡º JSON
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(test_report, f, ensure_ascii=False, indent=2)

        print(f"âœ… æ¸¬è©¦çµæœå·²åŒ¯å‡ºè‡³: {output_file}")
        print(f"   åŒ…å« {len(all_test_results)} è¼ªæ¸¬è©¦çµæœ")
        print("=" * 70)
    except Exception as e:
        print(f"âŒ åŒ¯å‡ºæ¸¬è©¦çµæœæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        print("=" * 70)

    # ä¿®æ”¹ï¼šç§»é™¤ä½åå·®å€¼ç¯©é¸å’Œ HTTP QPS Top 20 åˆ†æï¼ˆå·²æ”¹ç‚ºåœ¨ export_throughput_metrics ä¸­ç›´æ¥é€²è¡Œç¯©é¸ä¸¦è¦†è“‹åŸæª”æ¡ˆï¼‰
    # åŸç¨‹å¼ç¢¼ï¼ˆå·²è¨»é‡‹ï¼‰ï¼š
    # ==========================================
    # åŸ·è¡Œä½åå·®å€¼ç¯©é¸
    # ==========================================
    # if EXPORT_METRICS and os.path.exists(METRICS_OUTPUT_FILE):
    #     try:
    #         print("\n" + "=" * 70)
    #         print("  ğŸ” åŸ·è¡Œä½åå·®å€¼ç¯©é¸ï¼ˆåŸºæ–¼ logs per second ä¸­ä½æ•¸ï¼‰")
    #         print("=" * 70)
    #
    #         exporter = PrometheusExporter(PROMETHEUS_URL)
    #         filter_result = exporter.filter_logs_per_second_by_median(METRICS_OUTPUT_FILE)
    #
    #         if filter_result:
    #             print("\nğŸ“ˆ ç¯©é¸çµæœæ‘˜è¦:")
    #             print(f"  â€¢ åŸå§‹è³‡æ–™ç­†æ•¸: {filter_result['original_count']}")
    #             print(f"  â€¢ ç¯©é¸å¾Œç­†æ•¸: {filter_result['filtered_count']}")
    #             print(f"  â€¢ ä¿ç•™æ¯”ä¾‹: {filter_result['filtered_count']/filter_result['original_count']*100:.1f}%")
    #             print(f"  â€¢ ç¯©é¸é–¾å€¼ (ä¸­ä½æ•¸): {filter_result['median_threshold']:.2f}")
    #             print(f"  â€¢ åŸå§‹å¹³å‡å€¼: {filter_result['original_mean']:.2f}")
    #             print(f"  â€¢ ç¯©é¸å¾Œå¹³å‡å€¼: {filter_result['filtered_mean']:.2f}")
    #             print(f"  â€¢ å¹³å‡å€¼æå‡: {(filter_result['filtered_mean']-filter_result['original_mean'])/filter_result['original_mean']*100:.1f}%")
    #             print(f"  â€¢ ç¯©é¸å¾Œæª”æ¡ˆ: {filter_result['output_file']}")
    #
    #         print("=" * 70)
    #     except Exception as e:
    #         print(f"\nâš ï¸  ä½åå·®å€¼ç¯©é¸å¤±æ•—: {e}")
    #         print("   è«‹ç¢ºèª pandas å·²å®‰è£: pip install pandas")
    #
    # ==========================================
    # æ–°å¢åŠŸèƒ½ï¼šåŸ·è¡Œ HTTP QPS Top 20 åˆ†æ
    # ==========================================
    # ä¿®æ”¹èªªæ˜ï¼šåŒ¯å‡ºå®Œæˆå¾Œï¼Œè‡ªå‹•é€²è¡Œ HTTP QPS Top 20 åˆ†æ
    # if EXPORT_METRICS and os.path.exists(METRICS_OUTPUT_FILE):
    #     try:
    #         exporter = PrometheusExporter(PROMETHEUS_URL)
    #         exporter.filter_http_qps_top20(METRICS_OUTPUT_FILE)
    #     except Exception as e:
    #         print(f"\nâš ï¸  HTTP QPS Top 20 åˆ†æå¤±æ•—: {e}")
    #         print("   ä¸»è¦åŒ¯å‡ºæª”æ¡ˆä¸å—å½±éŸ¿")

    print("\nâœ… æ¸¬è©¦å®Œæˆ")

if __name__ == "__main__":
    asyncio.run(main())
